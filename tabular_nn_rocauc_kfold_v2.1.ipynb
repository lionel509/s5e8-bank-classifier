{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef51d9c",
   "metadata": {},
   "source": [
    "# High-ROC AUC Tabular NN â€” v2.1 Live + Stronger Generalization\n",
    "\n",
    "Upgrades focused on **higher ROC AUC** and fixing the **MPS `pin_memory` warning**:\n",
    "\n",
    "**What's new**\n",
    "- **MPS-safe data loading**: auto-detect Apple Silicon (MPS) and set `pin_memory=False`.\n",
    "- **Model**: wider MLP, **BatchNorm1d**, **GELU** activation, **Embedding Dropout**, and **Input Dropout** for regularization.\n",
    "- **Training**: gradient clipping, **CosineAnnealingWarmRestarts** scheduler, and **SWA (Stochastic Weight Averaging)** per fold for a small generalization bump.\n",
    "- Includes the **live tracker**, fold logs, per-fold/OOF plots, and Kaggle-ready `submission.csv`.\n",
    "\n",
    "> Place `train.csv` / `test.csv` in the working directory. Adjust the CONFIG if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "COMPETITION_NAME = \"playground-series-s5e8\"  # label only\n",
    "ID_COL = \"id\"\n",
    "TARGET_COL = \"y\"\n",
    "\n",
    "# Files\n",
    "TRAIN_PATH = \"playground-series-s5e8/train.csv\"\n",
    "TEST_PATH = \"playground-series-s5e8/test.csv\" \n",
    "\n",
    "# CV\n",
    "N_SPLITS = 7                # more folds for better validation\n",
    "RANDOM_SEED = 2025\n",
    "\n",
    "# Model / Training\n",
    "BATCH_SIZE = 2048           # smaller batch for better generalization\n",
    "EPOCHS = 150                # more epochs for better convergence\n",
    "PATIENCE = 20               # more patience for better training\n",
    "BASE_LR = 5e-4              # lower learning rate for stability\n",
    "WEIGHT_DECAY = 1e-4         # stronger regularization\n",
    "HIDDEN_LAYERS = [2048, 1024, 512, 256, 128]  # deeper & wider network\n",
    "DROPOUT = 0.3               # higher dropout for regularization\n",
    "EMB_DROPOUT = 0.1           # higher embedding dropout\n",
    "INPUT_DROPOUT = 0.1         # higher input dropout\n",
    "USE_CLASS_WEIGHTS = True\n",
    "\n",
    "# Scheduler settings\n",
    "COSINE_T0 = 15              # longer initial period\n",
    "COSINE_T_MULT = 2           # CosineAnnealingWarmRestarts T_mult\n",
    "MIN_LR = 1e-7               # lower minimum learning rate\n",
    "\n",
    "# SWA (Stochastic Weight Averaging) settings\n",
    "USE_SWA = True\n",
    "SWA_START_EPOCH = 30        # start SWA later for better base training\n",
    "SWA_LR = 2e-5               # lower SWA learning rate\n",
    "\n",
    "# Live tracking settings\n",
    "LIVE_PRINT = True           # print live updates\n",
    "LIVE_WRITE_EVERY = 1        # write to CSV every N epochs\n",
    "\n",
    "# Output (auto-filled below)\n",
    "RUN_STAMP = None\n",
    "RUN_DIR = None               # root for this run\n",
    "FIGS_DIR = None\n",
    "FOLDS_DIR = None\n",
    "LOGS_DIR = None\n",
    "MODELS_DIR = None\n",
    "ARTIFACTS_DIR = None\n",
    "\n",
    "VERBOSE_EVERY = 1\n",
    "SAVE_OOF = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56243681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps | pin_memory: False\n",
      "Run folder: runs/2025-08-12_10-17-24\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# IMPORTS & FOLDERS\n",
    "# =========================\n",
    "import os, gc, math, random, json, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# Device detection with MPS handling\n",
    "use_mps = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "if has_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# pin_memory only helps on CUDA; disable on MPS/CPU to avoid warnings\n",
    "pin_mem = True if device.type == \"cuda\" else False\n",
    "\n",
    "print(\"Device:\", device, \"| pin_memory:\", pin_mem)\n",
    "\n",
    "# Run folders\n",
    "RUN_STAMP = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "RUN_DIR = Path(f\"runs/{RUN_STAMP}\")\n",
    "FIGS_DIR = RUN_DIR / \"figs\"\n",
    "FOLDS_DIR = RUN_DIR / \"folds\"\n",
    "LOGS_DIR = RUN_DIR / \"logs\"\n",
    "MODELS_DIR = RUN_DIR / \"models\"\n",
    "ARTIFACTS_DIR = RUN_DIR / \"artifacts\"\n",
    "for d in [RUN_DIR, FIGS_DIR, FOLDS_DIR, LOGS_DIR, MODELS_DIR, ARTIFACTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "cfg = {k:v for k,v in dict(globals()).items() if k in [\n",
    "    \"COMPETITION_NAME\",\"ID_COL\",\"TARGET_COL\",\"TRAIN_PATH\",\"TEST_PATH\",\n",
    "    \"N_SPLITS\",\"RANDOM_SEED\",\"BATCH_SIZE\",\"EPOCHS\",\"PATIENCE\",\n",
    "    \"BASE_LR\",\"WEIGHT_DECAY\",\"HIDDEN_LAYERS\",\"DROPOUT\",\"EMB_DROPOUT\",\"INPUT_DROPOUT\",\n",
    "    \"USE_CLASS_WEIGHTS\",\"COSINE_T0\",\"COSINE_T_MULT\",\"MIN_LR\",\"USE_SWA\",\"SWA_START_EPOCH\",\"SWA_LR\",\n",
    "    \"LIVE_PRINT\",\"LIVE_WRITE_EVERY\",\"RUN_STAMP\"\n",
    "]}\n",
    "with open(RUN_DIR / \"config.json\", \"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"Run folder:\", RUN_DIR.as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f68fdb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (750000, 18) Test: (250000, 17)\n",
      "Target positive rate: 0.120651\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "print(\"Train:\", train.shape, \"Test:\", test.shape)\n",
    "\n",
    "assert TARGET_COL in train.columns, f\"TARGET_COL '{TARGET_COL}' missing\"\n",
    "assert ID_COL in train.columns, f\"ID_COL '{ID_COL}' missing\"\n",
    "assert ID_COL in test.columns, f\"ID_COL '{ID_COL}' missing in test\"\n",
    "\n",
    "feature_cols = [c for c in train.columns if c not in [TARGET_COL, ID_COL]]\n",
    "missing_in_test = [c for c in feature_cols if c not in test.columns]\n",
    "assert not missing_in_test, f\"Missing features in test: {missing_in_test}\"\n",
    "\n",
    "y = train[TARGET_COL].values\n",
    "print(\"Target positive rate:\", y.mean().round(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55d9ee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoricals (9): ['contact', 'default', 'education', 'housing', 'job', 'loan', 'marital', 'month', 'poutcome']\n",
      "Numerics (7): ['age', 'balance', 'campaign', 'day', 'duration', 'pdays', 'previous']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 2, 4, 2, 12, 2, 3, 12, 4]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# FEATURE TYPING & PREPROCESS\n",
    "# =========================\n",
    "obj_cols = [c for c in feature_cols if train[c].dtype == 'object']\n",
    "lowcard_int_cols = [c for c in feature_cols \n",
    "                    if str(train[c].dtype).startswith('int') and train[c].nunique() <= 30]\n",
    "cat_cols = sorted(list(set(obj_cols + lowcard_int_cols)))\n",
    "num_cols = sorted([c for c in feature_cols if c not in cat_cols])\n",
    "print(f\"Categoricals ({len(cat_cols)}):\", cat_cols[:20])\n",
    "print(f\"Numerics ({len(num_cols)}):\", num_cols[:20])\n",
    "\n",
    "# Rare category handling\n",
    "RARE_NAME = \"__RARE__\"\n",
    "MIN_CAT_COUNT = 10          # lower threshold to preserve more categories\n",
    "\n",
    "def apply_rare(series: pd.Series, min_count: int = MIN_CAT_COUNT) -> pd.Series:\n",
    "    counts = series.value_counts()\n",
    "    rare = counts[counts < min_count].index\n",
    "    return series.where(~series.isin(rare), RARE_NAME)\n",
    "\n",
    "encoders = {}\n",
    "for c in cat_cols:\n",
    "    s_tr = apply_rare(train[c].astype(str))\n",
    "    s_te = apply_rare(test[c].astype(str))\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([s_tr, s_te], axis=0).fillna(\"NA\"))\n",
    "    encoders[c] = le\n",
    "    train[c] = le.transform(s_tr.fillna(\"NA\"))\n",
    "    test[c]  = le.transform(s_te.fillna(\"NA\"))\n",
    "\n",
    "scaler = None\n",
    "if len(num_cols) > 0:\n",
    "    scaler = StandardScaler()\n",
    "    train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "    test[num_cols] = scaler.transform(test[num_cols])\n",
    "\n",
    "cat_cardinalities = [int(train[c].nunique()) for c in cat_cols]\n",
    "cat_cardinalities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95d2fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, df, y=None, num_cols=None, cat_cols=None):\n",
    "        self.num = df[num_cols].values.astype(np.float32) if num_cols else np.zeros((len(df),0), np.float32)\n",
    "        self.cat = df[cat_cols].values.astype(np.int64) if cat_cols else np.zeros((len(df),0), np.int64)\n",
    "        self.y = y.astype(np.float32) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.num[idx], self.cat[idx]\n",
    "        return self.num[idx], self.cat[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e49963a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MODEL (GELU + BatchNorm + Embedding Dropout + Input Dropout)\n",
    "# =========================\n",
    "class TabularNN(nn.Module):\n",
    "    def __init__(self, num_dim, cat_cardinalities, hidden_layers, dropout=0.25, emb_dropout=0.05, input_dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.has_cat = len(cat_cardinalities) > 0\n",
    "        self.has_num = num_dim > 0\n",
    "        self.input_dropout = nn.Dropout(input_dropout) if input_dropout > 0 and self.has_num else nn.Identity()\n",
    "\n",
    "        # Embeddings\n",
    "        if self.has_cat:\n",
    "            emb_dims = []\n",
    "            self.emb_layers = nn.ModuleList()\n",
    "            for card in cat_cardinalities:\n",
    "                emb_dim = int(min(64, max(4, round(1.6 * (card ** 0.56)))))  # slightly larger cap\n",
    "                self.emb_layers.append(nn.Embedding(card, emb_dim))\n",
    "                emb_dims.append(emb_dim)\n",
    "            self.emb_dropout = nn.Dropout(emb_dropout) if emb_dropout > 0 else nn.Identity()\n",
    "            emb_total = sum(emb_dims)\n",
    "        else:\n",
    "            self.emb_layers = None\n",
    "            self.emb_dropout = nn.Identity()\n",
    "            emb_total = 0\n",
    "\n",
    "        in_dim = (num_dim if self.has_num else 0) + emb_total\n",
    "\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden_layers:\n",
    "            layers += [\n",
    "                nn.Linear(prev, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            prev = h\n",
    "        layers += [nn.Linear(prev, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        feats = []\n",
    "\n",
    "        if self.has_cat:\n",
    "            embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.emb_layers)]\n",
    "            cat_feat = torch.cat(embs, dim=1)\n",
    "            cat_feat = self.emb_dropout(cat_feat)\n",
    "            feats.append(cat_feat)\n",
    "\n",
    "        if self.has_num:\n",
    "            x_num = self.input_dropout(x_num)\n",
    "            feats.append(x_num)\n",
    "\n",
    "        x = torch.cat(feats, dim=1) if len(feats) > 1 else feats[0]\n",
    "        logit = self.mlp(x).squeeze(1)\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47b8e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LIVE TRACKER\n",
    "# =========================\n",
    "class LiveTracker:\n",
    "    def __init__(self, logs_dir: Path, write_every=1, live_print=True):\n",
    "        self.logs_dir = Path(logs_dir)\n",
    "        self.logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.csv_path = self.logs_dir / \"live_status.csv\"\n",
    "        self.json_path = self.logs_dir / \"status.json\"\n",
    "        self.write_every = max(1, int(write_every))\n",
    "        self.live_print = live_print\n",
    "        self.rows = []\n",
    "        if not self.csv_path.exists():\n",
    "            with open(self.csv_path, \"w\") as f:\n",
    "                f.write(\"timestamp,fold,epoch,train_loss,val_loss,val_auc,lr,best_val_auc,elapsed_s\\n\")\n",
    "\n",
    "    def update(self, fold, epoch, train_loss, val_loss, val_auc, lr, best_val_auc, t0):\n",
    "        now = time.time()\n",
    "        row = dict(\n",
    "            timestamp=datetime.now(timezone.utc).isoformat(),\n",
    "            fold=fold, epoch=epoch,\n",
    "            train_loss=float(train_loss) if train_loss is not None else None,\n",
    "            val_loss=float(val_loss) if val_loss is not None else None,\n",
    "            val_auc=float(val_auc) if val_auc is not None else None,\n",
    "            lr=float(lr), best_val_auc=float(best_val_auc) if best_val_auc is not None else None,\n",
    "            elapsed_s=now - t0\n",
    "        )\n",
    "        self.rows.append(row)\n",
    "        if self.live_print:\n",
    "            print(f\"[fold {fold} | ep {epoch:03d}] tr={train_loss:.5f} va={val_loss:.5f} AUC={val_auc:.6f} best={best_val_auc:.6f} lr={lr:.2e}\")\n",
    "        if len(self.rows) % self.write_every == 0:\n",
    "            self.flush()\n",
    "        with open(self.json_path, \"w\") as f:\n",
    "            json.dump(row, f, indent=2)\n",
    "\n",
    "    def flush(self):\n",
    "        with open(self.csv_path, \"a\") as f:\n",
    "            for r in self.rows:\n",
    "                f.write(\",\".join([\n",
    "                    r[\"timestamp\"], str(r[\"fold\"]), str(r[\"epoch\"]),\n",
    "                    f\"{r['train_loss']:.6f}\" if r[\"train_loss\"] is not None else \"\",\n",
    "                    f\"{r['val_loss']:.6f}\" if r[\"val_loss\"] is not None else \"\",\n",
    "                    f\"{r['val_auc']:.6f}\" if r[\"val_auc\"] is not None else \"\",\n",
    "                    f\"{r['lr']:.6e}\", f\"{r['best_val_auc']:.6f}\" if r[\"best_val_auc\"] is not None else \"\",\n",
    "                    f\"{r['elapsed_s']:.2f}\"\n",
    "                ]) + \"\\n\")\n",
    "        self.rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c5d7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TRAINING HELPERS\n",
    "# =========================\n",
    "def epoch_loop(model, loader, criterion, optimizer=None, clip_grad=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "    losses, preds, targs = [], [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        if is_train:\n",
    "            x_num, x_cat, y = batch\n",
    "        else:\n",
    "            try:\n",
    "                x_num, x_cat, y = batch\n",
    "            except:\n",
    "                x_num, x_cat = batch\n",
    "                y = None\n",
    "\n",
    "        x_num = x_num.to(device)\n",
    "        x_cat = x_cat.to(device)\n",
    "        if y is not None:\n",
    "            y = y.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logit = model(x_num, x_cat)\n",
    "            prob = torch.sigmoid(logit)\n",
    "            loss = criterion(logit, y) if y is not None else None\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "\n",
    "        if loss is not None:\n",
    "            losses.append(loss.item())\n",
    "            targs.append(y.detach().cpu().numpy())\n",
    "        preds.append(prob.detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds) if preds else np.array([])\n",
    "    y_true = np.concatenate(targs) if targs else None\n",
    "    avg_loss = float(np.mean(losses)) if losses else None\n",
    "    return avg_loss, preds, y_true\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, mode=\"max\", min_delta=1e-6):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.best = -np.inf if mode==\"max\" else np.inf\n",
    "        self.count = 0\n",
    "        self.min_delta = min_delta\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metric, model):\n",
    "        improved = (metric > self.best + self.min_delta) if self.mode==\"max\" else (metric < self.best - self.min_delta)\n",
    "        if improved:\n",
    "            self.best = metric\n",
    "            self.count = 0\n",
    "            self.best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            return True\n",
    "        else:\n",
    "            self.count += 1\n",
    "            return False\n",
    "\n",
    "    def should_stop(self):\n",
    "        return self.count >= self.patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47f920cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 0 =====\n",
      "[fold 0 | ep 001] tr=0.55765 va=0.45439 AUC=0.955639 best=0.955639 lr=9.76e-04\n",
      "[fold 0 | ep 001] tr=0.55765 va=0.45439 AUC=0.955639 best=0.955639 lr=9.76e-04\n",
      "[fold 0 | ep 002] tr=0.49772 va=0.44086 AUC=0.958382 best=0.958382 lr=9.05e-04\n",
      "[fold 0 | ep 002] tr=0.49772 va=0.44086 AUC=0.958382 best=0.958382 lr=9.05e-04\n",
      "[fold 0 | ep 003] tr=0.48458 va=0.43695 AUC=0.959206 best=0.959206 lr=7.94e-04\n",
      "[fold 0 | ep 003] tr=0.48458 va=0.43695 AUC=0.959206 best=0.959206 lr=7.94e-04\n",
      "[fold 0 | ep 004] tr=0.47706 va=0.43520 AUC=0.959584 best=0.959584 lr=6.55e-04\n",
      "[fold 0 | ep 004] tr=0.47706 va=0.43520 AUC=0.959584 best=0.959584 lr=6.55e-04\n",
      "[fold 0 | ep 005] tr=0.47249 va=0.43641 AUC=0.959260 best=0.959584 lr=5.01e-04\n",
      "[fold 0 | ep 005] tr=0.47249 va=0.43641 AUC=0.959260 best=0.959584 lr=5.01e-04\n",
      "[fold 0 | ep 006] tr=0.47041 va=0.42991 AUC=0.960388 best=0.960388 lr=3.46e-04\n",
      "[fold 0 | ep 006] tr=0.47041 va=0.42991 AUC=0.960388 best=0.960388 lr=3.46e-04\n",
      "[fold 0 | ep 007] tr=0.46598 va=0.42876 AUC=0.960672 best=0.960672 lr=2.07e-04\n",
      "[fold 0 | ep 007] tr=0.46598 va=0.42876 AUC=0.960672 best=0.960672 lr=2.07e-04\n",
      "[fold 0 | ep 008] tr=0.46529 va=0.42854 AUC=0.960888 best=0.960888 lr=9.64e-05\n",
      "[fold 0 | ep 008] tr=0.46529 va=0.42854 AUC=0.960888 best=0.960888 lr=9.64e-05\n",
      "[fold 0 | ep 009] tr=0.46367 va=0.42730 AUC=0.960962 best=0.960962 lr=2.54e-05\n",
      "[fold 0 | ep 009] tr=0.46367 va=0.42730 AUC=0.960962 best=0.960962 lr=2.54e-05\n",
      "[fold 0 | ep 010] tr=0.46117 va=0.42730 AUC=0.961040 best=0.961040 lr=1.00e-03\n",
      "[fold 0 | ep 010] tr=0.46117 va=0.42730 AUC=0.961040 best=0.961040 lr=1.00e-03\n",
      "[fold 0 | ep 011] tr=0.47001 va=0.43061 AUC=0.960298 best=0.961040 lr=9.94e-04\n",
      "[fold 0 | ep 011] tr=0.47001 va=0.43061 AUC=0.960298 best=0.961040 lr=9.94e-04\n",
      "[fold 0 | ep 012] tr=0.46709 va=0.43214 AUC=0.960363 best=0.961040 lr=9.76e-04\n",
      "[fold 0 | ep 012] tr=0.46709 va=0.43214 AUC=0.960363 best=0.961040 lr=9.76e-04\n",
      "[fold 0 | ep 013] tr=0.46607 va=0.42885 AUC=0.960585 best=0.961040 lr=9.46e-04\n",
      "[fold 0 | ep 013] tr=0.46607 va=0.42885 AUC=0.960585 best=0.961040 lr=9.46e-04\n",
      "[fold 0 | ep 014] tr=0.46344 va=0.42926 AUC=0.960659 best=0.961040 lr=9.05e-04\n",
      "[fold 0 | ep 014] tr=0.46344 va=0.42926 AUC=0.960659 best=0.961040 lr=9.05e-04\n",
      "[fold 0 | ep 015] tr=0.46183 va=0.42968 AUC=0.960877 best=0.961040 lr=8.54e-04\n",
      "[fold 0 | ep 015] tr=0.46183 va=0.42968 AUC=0.960877 best=0.961040 lr=8.54e-04\n",
      "[fold 0 | ep 016] tr=0.46158 va=0.42699 AUC=0.961141 best=0.961141 lr=7.94e-04\n",
      "[fold 0 | ep 016] tr=0.46158 va=0.42699 AUC=0.961141 best=0.961141 lr=7.94e-04\n",
      "[fold 0 | ep 017] tr=0.45888 va=0.42761 AUC=0.960723 best=0.961141 lr=7.27e-04\n",
      "[fold 0 | ep 017] tr=0.45888 va=0.42761 AUC=0.960723 best=0.961141 lr=7.27e-04\n",
      "[fold 0 | ep 018] tr=0.45776 va=0.42662 AUC=0.961213 best=0.961213 lr=6.55e-04\n",
      "[fold 0 | ep 018] tr=0.45776 va=0.42662 AUC=0.961213 best=0.961213 lr=6.55e-04\n",
      "[fold 0 | ep 019] tr=0.45611 va=0.42635 AUC=0.961329 best=0.961329 lr=5.79e-04\n",
      "[fold 0 | ep 019] tr=0.45611 va=0.42635 AUC=0.961329 best=0.961329 lr=5.79e-04\n",
      "[fold 0 | ep 020] tr=0.45305 va=0.42437 AUC=0.961577 best=0.961577 lr=4.91e-04\n",
      "[fold 0 | ep 020] tr=0.45305 va=0.42437 AUC=0.961577 best=0.961577 lr=4.91e-04\n",
      "[fold 0 | ep 021] tr=0.45385 va=0.42636 AUC=0.961212 best=0.961577 lr=3.99e-04\n",
      "[fold 0 | ep 021] tr=0.45385 va=0.42636 AUC=0.961212 best=0.961577 lr=3.99e-04\n",
      "[fold 0 | ep 022] tr=0.45336 va=0.42320 AUC=0.961842 best=0.961842 lr=3.16e-04\n",
      "[fold 0 | ep 022] tr=0.45336 va=0.42320 AUC=0.961842 best=0.961842 lr=3.16e-04\n",
      "[fold 0 | ep 023] tr=0.45103 va=0.42367 AUC=0.961819 best=0.961842 lr=2.43e-04\n",
      "[fold 0 | ep 023] tr=0.45103 va=0.42367 AUC=0.961819 best=0.961842 lr=2.43e-04\n",
      "[fold 0 | ep 024] tr=0.45008 va=0.42335 AUC=0.961770 best=0.961842 lr=1.82e-04\n",
      "[fold 0 | ep 024] tr=0.45008 va=0.42335 AUC=0.961770 best=0.961842 lr=1.82e-04\n",
      "[fold 0 | ep 025] tr=0.44864 va=0.42194 AUC=0.962161 best=0.962161 lr=1.33e-04\n",
      "[fold 0 | ep 025] tr=0.44864 va=0.42194 AUC=0.962161 best=0.962161 lr=1.33e-04\n",
      "[fold 0 | ep 026] tr=0.44711 va=0.42119 AUC=0.962144 best=0.962161 lr=9.78e-05\n",
      "[fold 0 | ep 026] tr=0.44711 va=0.42119 AUC=0.962144 best=0.962161 lr=9.78e-05\n",
      "[fold 0 | ep 027] tr=0.44677 va=0.42185 AUC=0.962082 best=0.962161 lr=7.94e-05\n",
      "[fold 0 | ep 027] tr=0.44677 va=0.42185 AUC=0.962082 best=0.962161 lr=7.94e-05\n",
      "[fold 0 | ep 028] tr=0.44726 va=0.42158 AUC=0.962162 best=0.962162 lr=8.09e-05\n",
      "[fold 0 | ep 028] tr=0.44726 va=0.42158 AUC=0.962162 best=0.962162 lr=8.09e-05\n",
      "[fold 0 | ep 029] tr=0.44638 va=0.42166 AUC=0.962131 best=0.962162 lr=1.00e-04\n",
      "[fold 0 | ep 029] tr=0.44638 va=0.42166 AUC=0.962131 best=0.962162 lr=1.00e-04\n",
      "[fold 0 | ep 030] tr=0.44630 va=0.42197 AUC=0.962079 best=0.962162 lr=1.00e-04\n",
      "[fold 0 | ep 030] tr=0.44630 va=0.42197 AUC=0.962079 best=0.962162 lr=1.00e-04\n",
      "[fold 0 | ep 031] tr=0.44580 va=0.42147 AUC=0.962180 best=0.962180 lr=1.00e-04\n",
      "[fold 0 | ep 031] tr=0.44580 va=0.42147 AUC=0.962180 best=0.962180 lr=1.00e-04\n",
      "[fold 0 | ep 032] tr=0.44519 va=0.42148 AUC=0.962260 best=0.962260 lr=1.00e-04\n",
      "[fold 0 | ep 032] tr=0.44519 va=0.42148 AUC=0.962260 best=0.962260 lr=1.00e-04\n",
      "[fold 0 | ep 033] tr=0.44511 va=0.42167 AUC=0.962117 best=0.962260 lr=1.00e-04\n",
      "[fold 0 | ep 033] tr=0.44511 va=0.42167 AUC=0.962117 best=0.962260 lr=1.00e-04\n",
      "[fold 0 | ep 034] tr=0.44620 va=0.42121 AUC=0.962314 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 034] tr=0.44620 va=0.42121 AUC=0.962314 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 035] tr=0.44577 va=0.42287 AUC=0.962032 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 035] tr=0.44577 va=0.42287 AUC=0.962032 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 036] tr=0.44386 va=0.42176 AUC=0.962277 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 036] tr=0.44386 va=0.42176 AUC=0.962277 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 037] tr=0.44525 va=0.42158 AUC=0.962256 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 037] tr=0.44525 va=0.42158 AUC=0.962256 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 038] tr=0.44537 va=0.42206 AUC=0.962171 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 038] tr=0.44537 va=0.42206 AUC=0.962171 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 039] tr=0.44343 va=0.42173 AUC=0.962213 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 039] tr=0.44343 va=0.42173 AUC=0.962213 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 040] tr=0.44303 va=0.42084 AUC=0.962306 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 040] tr=0.44303 va=0.42084 AUC=0.962306 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 041] tr=0.44329 va=0.42137 AUC=0.962255 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 041] tr=0.44329 va=0.42137 AUC=0.962255 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 042] tr=0.44450 va=0.42225 AUC=0.962084 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 042] tr=0.44450 va=0.42225 AUC=0.962084 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 043] tr=0.44337 va=0.42183 AUC=0.962240 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 043] tr=0.44337 va=0.42183 AUC=0.962240 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 044] tr=0.44285 va=0.42185 AUC=0.962238 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 044] tr=0.44285 va=0.42185 AUC=0.962238 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 045] tr=0.44398 va=0.42141 AUC=0.962300 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 045] tr=0.44398 va=0.42141 AUC=0.962300 best=0.962314 lr=1.00e-04\n",
      "[fold 0 | ep 046] tr=0.44440 va=0.42096 AUC=0.962352 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 046] tr=0.44440 va=0.42096 AUC=0.962352 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 047] tr=0.44070 va=0.42215 AUC=0.962142 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 047] tr=0.44070 va=0.42215 AUC=0.962142 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 048] tr=0.44328 va=0.42291 AUC=0.962040 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 048] tr=0.44328 va=0.42291 AUC=0.962040 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 049] tr=0.44205 va=0.42217 AUC=0.962267 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 049] tr=0.44205 va=0.42217 AUC=0.962267 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 050] tr=0.44300 va=0.42134 AUC=0.962311 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 050] tr=0.44300 va=0.42134 AUC=0.962311 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 051] tr=0.44279 va=0.42172 AUC=0.962209 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 051] tr=0.44279 va=0.42172 AUC=0.962209 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 052] tr=0.44188 va=0.42122 AUC=0.962311 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 052] tr=0.44188 va=0.42122 AUC=0.962311 best=0.962352 lr=1.00e-04\n",
      "[fold 0 | ep 053] tr=0.43867 va=0.42139 AUC=0.962380 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 053] tr=0.43867 va=0.42139 AUC=0.962380 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 054] tr=0.44093 va=0.42171 AUC=0.962278 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 054] tr=0.44093 va=0.42171 AUC=0.962278 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 055] tr=0.44172 va=0.42216 AUC=0.962313 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 056] tr=0.44056 va=0.42169 AUC=0.962296 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 057] tr=0.44111 va=0.42293 AUC=0.962131 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 058] tr=0.44080 va=0.42263 AUC=0.962141 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 059] tr=0.44042 va=0.42263 AUC=0.962220 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 060] tr=0.43985 va=0.42320 AUC=0.962119 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 061] tr=0.44033 va=0.42241 AUC=0.962160 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 062] tr=0.43921 va=0.42328 AUC=0.962168 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 063] tr=0.44059 va=0.42324 AUC=0.962215 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 064] tr=0.43916 va=0.42399 AUC=0.962032 best=0.962380 lr=1.00e-04\n",
      "[fold 0 | ep 065] tr=0.43966 va=0.42345 AUC=0.962165 best=0.962380 lr=1.00e-04\n",
      "Early stopping at epoch 65. Best AUC: 0.962380\n",
      "[Interim] OOF ROC AUC using completed folds only will be printed at the end.\n",
      "\n",
      "===== Fold 1 =====\n",
      "[fold 1 | ep 001] tr=0.55722 va=0.45548 AUC=0.955344 best=0.955344 lr=9.05e-04\n",
      "[fold 1 | ep 002] tr=0.49211 va=0.44700 AUC=0.957337 best=0.957337 lr=7.94e-04\n",
      "[fold 1 | ep 003] tr=0.48065 va=0.44138 AUC=0.958592 best=0.958592 lr=6.55e-04\n",
      "[fold 1 | ep 004] tr=0.47268 va=0.43709 AUC=0.959197 best=0.959197 lr=5.01e-04\n",
      "[fold 1 | ep 005] tr=0.46907 va=0.43607 AUC=0.959487 best=0.959487 lr=3.46e-04\n",
      "[fold 1 | ep 006] tr=0.46604 va=0.43346 AUC=0.959802 best=0.959802 lr=2.07e-04\n",
      "[fold 1 | ep 007] tr=0.46421 va=0.43502 AUC=0.959724 best=0.959802 lr=9.64e-05\n",
      "[fold 1 | ep 008] tr=0.46262 va=0.43274 AUC=0.960078 best=0.960078 lr=2.54e-05\n",
      "[fold 1 | ep 009] tr=0.46024 va=0.43179 AUC=0.960182 best=0.960182 lr=1.00e-03\n",
      "[fold 1 | ep 010] tr=0.46960 va=0.43922 AUC=0.958924 best=0.960182 lr=9.94e-04\n",
      "[fold 1 | ep 011] tr=0.46612 va=0.43600 AUC=0.959416 best=0.960182 lr=9.76e-04\n",
      "[fold 1 | ep 012] tr=0.46412 va=0.43637 AUC=0.959586 best=0.960182 lr=9.46e-04\n",
      "[fold 1 | ep 013] tr=0.46294 va=0.43409 AUC=0.959930 best=0.960182 lr=9.05e-04\n",
      "[fold 1 | ep 014] tr=0.46062 va=0.43537 AUC=0.959830 best=0.960182 lr=8.54e-04\n",
      "[fold 1 | ep 015] tr=0.45833 va=0.43165 AUC=0.960453 best=0.960453 lr=7.94e-04\n",
      "[fold 1 | ep 016] tr=0.45825 va=0.43124 AUC=0.960624 best=0.960624 lr=7.27e-04\n",
      "[fold 1 | ep 017] tr=0.45662 va=0.43110 AUC=0.960419 best=0.960624 lr=6.55e-04\n",
      "[fold 1 | ep 018] tr=0.45575 va=0.43066 AUC=0.960560 best=0.960624 lr=5.79e-04\n",
      "[fold 1 | ep 019] tr=0.45325 va=0.42901 AUC=0.960977 best=0.960977 lr=5.01e-04\n",
      "[fold 1 | ep 020] tr=0.45263 va=0.43057 AUC=0.960758 best=0.960977 lr=4.14e-04\n",
      "[fold 1 | ep 021] tr=0.45135 va=0.42800 AUC=0.961066 best=0.961066 lr=3.28e-04\n",
      "[fold 1 | ep 022] tr=0.44930 va=0.42688 AUC=0.961220 best=0.961220 lr=2.52e-04\n",
      "[fold 1 | ep 023] tr=0.44759 va=0.42871 AUC=0.961004 best=0.961220 lr=1.88e-04\n",
      "[fold 1 | ep 024] tr=0.44616 va=0.42665 AUC=0.961318 best=0.961318 lr=1.36e-04\n",
      "[fold 1 | ep 025] tr=0.44769 va=0.42724 AUC=0.961223 best=0.961318 lr=9.75e-05\n",
      "[fold 1 | ep 026] tr=0.44563 va=0.42683 AUC=0.961340 best=0.961340 lr=7.34e-05\n",
      "[fold 1 | ep 027] tr=0.44384 va=0.42556 AUC=0.961565 best=0.961565 lr=6.55e-05\n",
      "[fold 1 | ep 028] tr=0.44537 va=0.42706 AUC=0.961371 best=0.961565 lr=7.62e-05\n",
      "[fold 1 | ep 029] tr=0.44378 va=0.42641 AUC=0.961413 best=0.961565 lr=1.00e-04\n",
      "[fold 1 | ep 030] tr=0.44372 va=0.42623 AUC=0.961424 best=0.961565 lr=1.00e-04\n",
      "[fold 1 | ep 031] tr=0.44431 va=0.42607 AUC=0.961539 best=0.961565 lr=1.00e-04\n",
      "[fold 1 | ep 032] tr=0.44385 va=0.42577 AUC=0.961616 best=0.961616 lr=1.00e-04\n",
      "[fold 1 | ep 033] tr=0.44448 va=0.42550 AUC=0.961611 best=0.961616 lr=1.00e-04\n",
      "[fold 1 | ep 034] tr=0.44417 va=0.42561 AUC=0.961565 best=0.961616 lr=1.00e-04\n",
      "[fold 1 | ep 035] tr=0.44472 va=0.42731 AUC=0.961235 best=0.961616 lr=1.00e-04\n",
      "[fold 1 | ep 036] tr=0.44401 va=0.42630 AUC=0.961482 best=0.961616 lr=1.00e-04\n",
      "[fold 1 | ep 037] tr=0.44201 va=0.42599 AUC=0.961544 best=0.961616 lr=1.00e-04\n",
      "[fold 1 | ep 038] tr=0.44143 va=0.42560 AUC=0.961602 best=0.961616 lr=1.00e-04\n",
      "[fold 1 | ep 039] tr=0.44184 va=0.42506 AUC=0.961744 best=0.961744 lr=1.00e-04\n",
      "[fold 1 | ep 040] tr=0.44271 va=0.42538 AUC=0.961693 best=0.961744 lr=1.00e-04\n",
      "[fold 1 | ep 041] tr=0.44145 va=0.42499 AUC=0.961746 best=0.961746 lr=1.00e-04\n",
      "[fold 1 | ep 042] tr=0.44338 va=0.42574 AUC=0.961665 best=0.961746 lr=1.00e-04\n",
      "[fold 1 | ep 043] tr=0.44070 va=0.42622 AUC=0.961561 best=0.961746 lr=1.00e-04\n",
      "[fold 1 | ep 044] tr=0.44101 va=0.42467 AUC=0.961832 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 045] tr=0.44140 va=0.42719 AUC=0.961521 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 046] tr=0.44008 va=0.42657 AUC=0.961578 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 047] tr=0.44093 va=0.42592 AUC=0.961584 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 048] tr=0.44080 va=0.42595 AUC=0.961636 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 049] tr=0.43963 va=0.42627 AUC=0.961668 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 050] tr=0.44050 va=0.42671 AUC=0.961543 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 051] tr=0.43984 va=0.42637 AUC=0.961618 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 052] tr=0.43800 va=0.42638 AUC=0.961709 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 053] tr=0.43972 va=0.42653 AUC=0.961778 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 054] tr=0.43980 va=0.42572 AUC=0.961707 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 055] tr=0.43889 va=0.42576 AUC=0.961756 best=0.961832 lr=1.00e-04\n",
      "[fold 1 | ep 056] tr=0.43908 va=0.42620 AUC=0.961665 best=0.961832 lr=1.00e-04\n",
      "Early stopping at epoch 56. Best AUC: 0.961832\n",
      "[Interim] OOF ROC AUC using completed folds only will be printed at the end.\n",
      "\n",
      "===== Fold 2 =====\n",
      "[fold 2 | ep 001] tr=0.54844 va=0.45208 AUC=0.955889 best=0.955889 lr=7.94e-04\n",
      "[fold 2 | ep 002] tr=0.49113 va=0.44373 AUC=0.957795 best=0.957795 lr=6.55e-04\n",
      "[fold 2 | ep 003] tr=0.48203 va=0.44097 AUC=0.958560 best=0.958560 lr=5.01e-04\n",
      "[fold 2 | ep 004] tr=0.47510 va=0.43707 AUC=0.959294 best=0.959294 lr=3.46e-04\n",
      "[fold 2 | ep 005] tr=0.47001 va=0.43376 AUC=0.959931 best=0.959931 lr=2.07e-04\n",
      "[fold 2 | ep 006] tr=0.46732 va=0.43476 AUC=0.959683 best=0.959931 lr=9.64e-05\n",
      "[fold 2 | ep 007] tr=0.46636 va=0.43112 AUC=0.960212 best=0.960212 lr=2.54e-05\n",
      "[fold 2 | ep 008] tr=0.46397 va=0.43104 AUC=0.960262 best=0.960262 lr=1.00e-03\n",
      "[fold 2 | ep 009] tr=0.47366 va=0.43528 AUC=0.959577 best=0.960262 lr=9.94e-04\n",
      "[fold 2 | ep 010] tr=0.46991 va=0.43735 AUC=0.959186 best=0.960262 lr=9.76e-04\n",
      "[fold 2 | ep 011] tr=0.46607 va=0.43224 AUC=0.960289 best=0.960289 lr=9.46e-04\n",
      "[fold 2 | ep 012] tr=0.46534 va=0.43758 AUC=0.959549 best=0.960289 lr=9.05e-04\n",
      "[fold 2 | ep 013] tr=0.46300 va=0.43360 AUC=0.959911 best=0.960289 lr=8.54e-04\n",
      "[fold 2 | ep 014] tr=0.46178 va=0.43013 AUC=0.960537 best=0.960537 lr=7.94e-04\n",
      "[fold 2 | ep 015] tr=0.46029 va=0.43098 AUC=0.960464 best=0.960537 lr=7.27e-04\n",
      "[fold 2 | ep 016] tr=0.45885 va=0.43194 AUC=0.960091 best=0.960537 lr=6.55e-04\n",
      "[fold 2 | ep 017] tr=0.45598 va=0.43106 AUC=0.960320 best=0.960537 lr=5.79e-04\n",
      "[fold 2 | ep 018] tr=0.45509 va=0.42773 AUC=0.960968 best=0.960968 lr=5.01e-04\n",
      "[fold 2 | ep 019] tr=0.45384 va=0.42819 AUC=0.960948 best=0.960968 lr=4.22e-04\n",
      "[fold 2 | ep 020] tr=0.45285 va=0.42876 AUC=0.960968 best=0.960968 lr=3.40e-04\n",
      "[fold 2 | ep 021] tr=0.45217 va=0.42821 AUC=0.961082 best=0.961082 lr=2.61e-04\n",
      "[fold 2 | ep 022] tr=0.44932 va=0.42664 AUC=0.961289 best=0.961289 lr=1.94e-04\n",
      "[fold 2 | ep 023] tr=0.44699 va=0.42557 AUC=0.961552 best=0.961552 lr=1.39e-04\n",
      "[fold 2 | ep 024] tr=0.44693 va=0.42539 AUC=0.961621 best=0.961621 lr=9.72e-05\n",
      "[fold 2 | ep 025] tr=0.44686 va=0.42601 AUC=0.961474 best=0.961621 lr=6.92e-05\n",
      "[fold 2 | ep 026] tr=0.44628 va=0.42545 AUC=0.961552 best=0.961621 lr=5.55e-05\n",
      "[fold 2 | ep 027] tr=0.44539 va=0.42544 AUC=0.961575 best=0.961621 lr=5.70e-05\n",
      "[fold 2 | ep 028] tr=0.44468 va=0.42667 AUC=0.961373 best=0.961621 lr=3.31e-04\n",
      "[fold 2 | ep 029] tr=0.44806 va=0.42734 AUC=0.961287 best=0.961621 lr=1.00e-04\n",
      "[fold 2 | ep 030] tr=0.44475 va=0.42597 AUC=0.961534 best=0.961621 lr=1.00e-04\n",
      "[fold 2 | ep 031] tr=0.44406 va=0.42533 AUC=0.961697 best=0.961697 lr=1.00e-04\n",
      "[fold 2 | ep 032] tr=0.44540 va=0.42721 AUC=0.961240 best=0.961697 lr=1.00e-04\n",
      "[fold 2 | ep 033] tr=0.44421 va=0.42694 AUC=0.961462 best=0.961697 lr=1.00e-04\n",
      "[fold 2 | ep 034] tr=0.44373 va=0.42614 AUC=0.961542 best=0.961697 lr=1.00e-04\n",
      "[fold 2 | ep 035] tr=0.44469 va=0.42600 AUC=0.961545 best=0.961697 lr=1.00e-04\n",
      "[fold 2 | ep 036] tr=0.44245 va=0.42573 AUC=0.961650 best=0.961697 lr=1.00e-04\n",
      "[fold 2 | ep 037] tr=0.44411 va=0.42612 AUC=0.961552 best=0.961697 lr=1.00e-04\n",
      "[fold 2 | ep 038] tr=0.44265 va=0.42642 AUC=0.961492 best=0.961697 lr=1.00e-04\n",
      "[fold 2 | ep 039] tr=0.44274 va=0.42500 AUC=0.961719 best=0.961719 lr=1.00e-04\n",
      "[fold 2 | ep 040] tr=0.44267 va=0.42601 AUC=0.961648 best=0.961719 lr=1.00e-04\n",
      "[fold 2 | ep 041] tr=0.44231 va=0.42541 AUC=0.961705 best=0.961719 lr=1.00e-04\n",
      "[fold 2 | ep 042] tr=0.44222 va=0.42633 AUC=0.961597 best=0.961719 lr=1.00e-04\n",
      "[fold 2 | ep 043] tr=0.44277 va=0.42579 AUC=0.961619 best=0.961719 lr=1.00e-04\n",
      "[fold 2 | ep 044] tr=0.44212 va=0.42661 AUC=0.961656 best=0.961719 lr=1.00e-04\n",
      "[fold 2 | ep 045] tr=0.44143 va=0.42603 AUC=0.961590 best=0.961719 lr=1.00e-04\n",
      "[fold 2 | ep 046] tr=0.44117 va=0.42613 AUC=0.961753 best=0.961753 lr=1.00e-04\n",
      "[fold 2 | ep 047] tr=0.44219 va=0.42651 AUC=0.961545 best=0.961753 lr=1.00e-04\n",
      "[fold 2 | ep 048] tr=0.44097 va=0.42508 AUC=0.961812 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 049] tr=0.44034 va=0.42597 AUC=0.961734 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 050] tr=0.44048 va=0.42724 AUC=0.961507 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 051] tr=0.44030 va=0.42665 AUC=0.961623 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 052] tr=0.44064 va=0.42618 AUC=0.961676 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 053] tr=0.44009 va=0.42610 AUC=0.961652 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 054] tr=0.43955 va=0.42721 AUC=0.961565 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 055] tr=0.43912 va=0.42635 AUC=0.961702 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 056] tr=0.43913 va=0.42761 AUC=0.961586 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 057] tr=0.43940 va=0.42734 AUC=0.961555 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 058] tr=0.43913 va=0.42665 AUC=0.961708 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 059] tr=0.43909 va=0.42676 AUC=0.961681 best=0.961812 lr=1.00e-04\n",
      "[fold 2 | ep 060] tr=0.43881 va=0.42697 AUC=0.961598 best=0.961812 lr=1.00e-04\n",
      "Early stopping at epoch 60. Best AUC: 0.961812\n",
      "[Interim] OOF ROC AUC using completed folds only will be printed at the end.\n",
      "\n",
      "===== Fold 3 =====\n",
      "[fold 3 | ep 001] tr=0.56318 va=0.45819 AUC=0.955918 best=0.955918 lr=6.55e-04\n",
      "[fold 3 | ep 002] tr=0.49360 va=0.44502 AUC=0.957886 best=0.957886 lr=5.01e-04\n",
      "[fold 3 | ep 003] tr=0.48155 va=0.43460 AUC=0.959260 best=0.959260 lr=3.46e-04\n",
      "[fold 3 | ep 004] tr=0.47675 va=0.43456 AUC=0.959695 best=0.959695 lr=2.07e-04\n",
      "[fold 3 | ep 005] tr=0.47374 va=0.43224 AUC=0.959965 best=0.959965 lr=9.64e-05\n",
      "[fold 3 | ep 006] tr=0.46993 va=0.42913 AUC=0.960433 best=0.960433 lr=2.54e-05\n",
      "[fold 3 | ep 007] tr=0.46976 va=0.42925 AUC=0.960474 best=0.960474 lr=1.00e-03\n",
      "[fold 3 | ep 008] tr=0.47790 va=0.43853 AUC=0.959713 best=0.960474 lr=9.94e-04\n",
      "[fold 3 | ep 009] tr=0.47220 va=0.42953 AUC=0.960270 best=0.960474 lr=9.76e-04\n",
      "[fold 3 | ep 010] tr=0.47044 va=0.43161 AUC=0.960246 best=0.960474 lr=9.46e-04\n",
      "[fold 3 | ep 011] tr=0.46816 va=0.42986 AUC=0.960467 best=0.960474 lr=9.05e-04\n",
      "[fold 3 | ep 012] tr=0.46511 va=0.42983 AUC=0.960649 best=0.960649 lr=8.54e-04\n",
      "[fold 3 | ep 013] tr=0.46228 va=0.42842 AUC=0.960760 best=0.960760 lr=7.94e-04\n",
      "[fold 3 | ep 014] tr=0.46148 va=0.42623 AUC=0.961002 best=0.961002 lr=7.27e-04\n",
      "[fold 3 | ep 015] tr=0.46031 va=0.42530 AUC=0.961245 best=0.961245 lr=6.55e-04\n",
      "[fold 3 | ep 016] tr=0.45794 va=0.42381 AUC=0.961460 best=0.961460 lr=5.79e-04\n",
      "[fold 3 | ep 017] tr=0.45694 va=0.42238 AUC=0.961637 best=0.961637 lr=5.01e-04\n",
      "[fold 3 | ep 018] tr=0.45651 va=0.42207 AUC=0.961858 best=0.961858 lr=4.22e-04\n",
      "[fold 3 | ep 019] tr=0.45397 va=0.42348 AUC=0.961316 best=0.961858 lr=3.46e-04\n",
      "[fold 3 | ep 020] tr=0.45162 va=0.42490 AUC=0.961707 best=0.961858 lr=2.69e-04\n",
      "[fold 3 | ep 021] tr=0.45190 va=0.42035 AUC=0.962117 best=0.962117 lr=1.99e-04\n",
      "[fold 3 | ep 022] tr=0.45159 va=0.42116 AUC=0.961919 best=0.962117 lr=1.42e-04\n",
      "[fold 3 | ep 023] tr=0.45029 va=0.42064 AUC=0.962142 best=0.962142 lr=9.70e-05\n",
      "[fold 3 | ep 024] tr=0.44849 va=0.42106 AUC=0.962073 best=0.962142 lr=6.60e-05\n",
      "[fold 3 | ep 025] tr=0.44821 va=0.41940 AUC=0.962288 best=0.962288 lr=4.85e-05\n",
      "[fold 3 | ep 026] tr=0.44692 va=0.41974 AUC=0.962242 best=0.962288 lr=4.46e-05\n",
      "[fold 3 | ep 027] tr=0.44799 va=0.41954 AUC=0.962274 best=0.962288 lr=5.17e-04\n",
      "[fold 3 | ep 028] tr=0.45198 va=0.42023 AUC=0.962093 best=0.962288 lr=3.30e-04\n",
      "[fold 3 | ep 029] tr=0.45099 va=0.42087 AUC=0.962038 best=0.962288 lr=1.00e-04\n",
      "[fold 3 | ep 030] tr=0.44823 va=0.41933 AUC=0.962292 best=0.962292 lr=1.00e-04\n",
      "[fold 3 | ep 031] tr=0.44793 va=0.41923 AUC=0.962392 best=0.962392 lr=1.00e-04\n",
      "[fold 3 | ep 032] tr=0.44557 va=0.41925 AUC=0.962355 best=0.962392 lr=1.00e-04\n",
      "[fold 3 | ep 033] tr=0.44644 va=0.41864 AUC=0.962410 best=0.962410 lr=1.00e-04\n",
      "[fold 3 | ep 034] tr=0.44669 va=0.41863 AUC=0.962480 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 035] tr=0.44492 va=0.41946 AUC=0.962358 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 036] tr=0.44547 va=0.41934 AUC=0.962323 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 037] tr=0.44444 va=0.41919 AUC=0.962375 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 038] tr=0.44429 va=0.41883 AUC=0.962394 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 039] tr=0.44475 va=0.41943 AUC=0.962363 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 040] tr=0.44503 va=0.41882 AUC=0.962406 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 041] tr=0.44523 va=0.41887 AUC=0.962445 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 042] tr=0.44468 va=0.42015 AUC=0.962298 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 043] tr=0.44450 va=0.41909 AUC=0.962401 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 044] tr=0.44296 va=0.41912 AUC=0.962431 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 045] tr=0.44371 va=0.41975 AUC=0.962373 best=0.962480 lr=1.00e-04\n",
      "[fold 3 | ep 046] tr=0.44311 va=0.41834 AUC=0.962523 best=0.962523 lr=1.00e-04\n",
      "[fold 3 | ep 047] tr=0.44326 va=0.41856 AUC=0.962514 best=0.962523 lr=1.00e-04\n",
      "[fold 3 | ep 048] tr=0.44324 va=0.41850 AUC=0.962499 best=0.962523 lr=1.00e-04\n",
      "[fold 3 | ep 049] tr=0.44168 va=0.42039 AUC=0.962386 best=0.962523 lr=1.00e-04\n",
      "[fold 3 | ep 050] tr=0.44233 va=0.41860 AUC=0.962546 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 051] tr=0.44213 va=0.41863 AUC=0.962508 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 052] tr=0.44186 va=0.41900 AUC=0.962496 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 053] tr=0.44113 va=0.41975 AUC=0.962449 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 054] tr=0.44241 va=0.41874 AUC=0.962522 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 055] tr=0.44263 va=0.41955 AUC=0.962515 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 056] tr=0.44023 va=0.41960 AUC=0.962499 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 057] tr=0.44001 va=0.42078 AUC=0.962390 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 058] tr=0.44072 va=0.41947 AUC=0.962488 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 059] tr=0.44192 va=0.41984 AUC=0.962491 best=0.962546 lr=1.00e-04\n",
      "[fold 3 | ep 060] tr=0.43921 va=0.41894 AUC=0.962548 best=0.962548 lr=1.00e-04\n",
      "[fold 3 | ep 061] tr=0.44139 va=0.41872 AUC=0.962588 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 062] tr=0.43978 va=0.42000 AUC=0.962442 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 063] tr=0.44022 va=0.41934 AUC=0.962517 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 064] tr=0.43904 va=0.41950 AUC=0.962465 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 065] tr=0.43909 va=0.41873 AUC=0.962532 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 066] tr=0.43850 va=0.41884 AUC=0.962588 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 067] tr=0.43914 va=0.41966 AUC=0.962560 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 068] tr=0.43959 va=0.42028 AUC=0.962426 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 069] tr=0.43811 va=0.41997 AUC=0.962454 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 070] tr=0.43817 va=0.41908 AUC=0.962567 best=0.962588 lr=1.00e-04\n",
      "[fold 3 | ep 071] tr=0.43946 va=0.41839 AUC=0.962614 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 072] tr=0.43808 va=0.41892 AUC=0.962571 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 073] tr=0.43866 va=0.41985 AUC=0.962411 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 074] tr=0.43789 va=0.41953 AUC=0.962474 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 075] tr=0.43702 va=0.41905 AUC=0.962543 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 076] tr=0.43891 va=0.42002 AUC=0.962382 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 077] tr=0.43777 va=0.41917 AUC=0.962557 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 078] tr=0.43717 va=0.41964 AUC=0.962590 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 079] tr=0.43672 va=0.41990 AUC=0.962592 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 080] tr=0.43653 va=0.42131 AUC=0.962459 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 081] tr=0.43733 va=0.41961 AUC=0.962536 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 082] tr=0.43545 va=0.42070 AUC=0.962539 best=0.962614 lr=1.00e-04\n",
      "[fold 3 | ep 083] tr=0.43497 va=0.42058 AUC=0.962375 best=0.962614 lr=1.00e-04\n",
      "Early stopping at epoch 83. Best AUC: 0.962614\n",
      "[Interim] OOF ROC AUC using completed folds only will be printed at the end.\n",
      "\n",
      "===== Fold 4 =====\n",
      "[fold 4 | ep 001] tr=0.55792 va=0.45867 AUC=0.955247 best=0.955247 lr=5.01e-04\n",
      "[fold 4 | ep 002] tr=0.49216 va=0.44386 AUC=0.958363 best=0.958363 lr=3.46e-04\n",
      "[fold 4 | ep 003] tr=0.48332 va=0.43872 AUC=0.959154 best=0.959154 lr=2.07e-04\n",
      "[fold 4 | ep 004] tr=0.47748 va=0.43531 AUC=0.959783 best=0.959783 lr=9.64e-05\n",
      "[fold 4 | ep 005] tr=0.47493 va=0.43419 AUC=0.960060 best=0.960060 lr=2.54e-05\n",
      "[fold 4 | ep 006] tr=0.47264 va=0.43426 AUC=0.959997 best=0.960060 lr=1.00e-03\n",
      "[fold 4 | ep 007] tr=0.48135 va=0.43991 AUC=0.959101 best=0.960060 lr=9.94e-04\n",
      "[fold 4 | ep 008] tr=0.47444 va=0.43519 AUC=0.959894 best=0.960060 lr=9.76e-04\n",
      "[fold 4 | ep 009] tr=0.47267 va=0.43716 AUC=0.959662 best=0.960060 lr=9.46e-04\n",
      "[fold 4 | ep 010] tr=0.46936 va=0.43311 AUC=0.960253 best=0.960253 lr=9.05e-04\n",
      "[fold 4 | ep 011] tr=0.46611 va=0.43794 AUC=0.959950 best=0.960253 lr=8.54e-04\n",
      "[fold 4 | ep 012] tr=0.46474 va=0.43119 AUC=0.960769 best=0.960769 lr=7.94e-04\n",
      "[fold 4 | ep 013] tr=0.46225 va=0.42849 AUC=0.961184 best=0.961184 lr=7.27e-04\n",
      "[fold 4 | ep 014] tr=0.46223 va=0.42820 AUC=0.961090 best=0.961184 lr=6.55e-04\n",
      "[fold 4 | ep 015] tr=0.45880 va=0.42911 AUC=0.961034 best=0.961184 lr=5.79e-04\n",
      "[fold 4 | ep 016] tr=0.45769 va=0.42977 AUC=0.960879 best=0.961184 lr=5.01e-04\n",
      "[fold 4 | ep 017] tr=0.45781 va=0.42704 AUC=0.961443 best=0.961443 lr=4.22e-04\n",
      "[fold 4 | ep 018] tr=0.45440 va=0.42632 AUC=0.961332 best=0.961443 lr=3.46e-04\n",
      "[fold 4 | ep 019] tr=0.45307 va=0.42437 AUC=0.961759 best=0.961759 lr=2.74e-04\n",
      "[fold 4 | ep 020] tr=0.45144 va=0.42493 AUC=0.961717 best=0.961759 lr=2.04e-04\n",
      "[fold 4 | ep 021] tr=0.45190 va=0.42598 AUC=0.961629 best=0.961759 lr=1.44e-04\n",
      "[fold 4 | ep 022] tr=0.44954 va=0.42482 AUC=0.961813 best=0.961813 lr=9.68e-05\n",
      "[fold 4 | ep 023] tr=0.44949 va=0.42421 AUC=0.961907 best=0.961907 lr=6.33e-05\n",
      "[fold 4 | ep 024] tr=0.44807 va=0.42536 AUC=0.961727 best=0.961907 lr=4.30e-05\n",
      "[fold 4 | ep 025] tr=0.44738 va=0.42472 AUC=0.961873 best=0.961907 lr=3.58e-05\n",
      "[fold 4 | ep 026] tr=0.44966 va=0.42459 AUC=0.961859 best=0.961907 lr=6.37e-04\n",
      "[fold 4 | ep 027] tr=0.45479 va=0.42873 AUC=0.961029 best=0.961907 lr=5.16e-04\n",
      "[fold 4 | ep 028] tr=0.45202 va=0.42684 AUC=0.961463 best=0.961907 lr=3.29e-04\n",
      "[fold 4 | ep 029] tr=0.45104 va=0.42690 AUC=0.961469 best=0.961907 lr=1.00e-04\n",
      "[fold 4 | ep 030] tr=0.44865 va=0.42439 AUC=0.961913 best=0.961913 lr=1.00e-04\n",
      "[fold 4 | ep 031] tr=0.44635 va=0.42508 AUC=0.961846 best=0.961913 lr=1.00e-04\n",
      "[fold 4 | ep 032] tr=0.44607 va=0.42409 AUC=0.961970 best=0.961970 lr=1.00e-04\n",
      "[fold 4 | ep 033] tr=0.44524 va=0.42526 AUC=0.961737 best=0.961970 lr=1.00e-04\n",
      "[fold 4 | ep 034] tr=0.44695 va=0.42469 AUC=0.961925 best=0.961970 lr=1.00e-04\n",
      "[fold 4 | ep 035] tr=0.44633 va=0.42502 AUC=0.961913 best=0.961970 lr=1.00e-04\n",
      "[fold 4 | ep 036] tr=0.44457 va=0.42425 AUC=0.961985 best=0.961985 lr=1.00e-04\n",
      "[fold 4 | ep 037] tr=0.44439 va=0.42557 AUC=0.961772 best=0.961985 lr=1.00e-04\n",
      "[fold 4 | ep 038] tr=0.44487 va=0.42512 AUC=0.961829 best=0.961985 lr=1.00e-04\n",
      "[fold 4 | ep 039] tr=0.44419 va=0.42441 AUC=0.962037 best=0.962037 lr=1.00e-04\n",
      "[fold 4 | ep 040] tr=0.44379 va=0.42443 AUC=0.961964 best=0.962037 lr=1.00e-04\n",
      "[fold 4 | ep 041] tr=0.44360 va=0.42433 AUC=0.962008 best=0.962037 lr=1.00e-04\n",
      "[fold 4 | ep 042] tr=0.44397 va=0.42470 AUC=0.962014 best=0.962037 lr=1.00e-04\n",
      "[fold 4 | ep 043] tr=0.44427 va=0.42454 AUC=0.962051 best=0.962051 lr=1.00e-04\n",
      "[fold 4 | ep 044] tr=0.44347 va=0.42401 AUC=0.962092 best=0.962092 lr=1.00e-04\n",
      "[fold 4 | ep 045] tr=0.44230 va=0.42449 AUC=0.961984 best=0.962092 lr=1.00e-04\n",
      "[fold 4 | ep 046] tr=0.44375 va=0.42489 AUC=0.961945 best=0.962092 lr=1.00e-04\n",
      "[fold 4 | ep 047] tr=0.44257 va=0.42450 AUC=0.962033 best=0.962092 lr=1.00e-04\n",
      "[fold 4 | ep 048] tr=0.44342 va=0.42414 AUC=0.962083 best=0.962092 lr=1.00e-04\n",
      "[fold 4 | ep 049] tr=0.44165 va=0.42399 AUC=0.962046 best=0.962092 lr=1.00e-04\n",
      "[fold 4 | ep 050] tr=0.44180 va=0.42528 AUC=0.961959 best=0.962092 lr=1.00e-04\n",
      "[fold 4 | ep 051] tr=0.44357 va=0.42439 AUC=0.962148 best=0.962148 lr=1.00e-04\n",
      "[fold 4 | ep 052] tr=0.44134 va=0.42433 AUC=0.962076 best=0.962148 lr=1.00e-04\n",
      "[fold 4 | ep 053] tr=0.44141 va=0.42481 AUC=0.962058 best=0.962148 lr=1.00e-04\n",
      "[fold 4 | ep 054] tr=0.44203 va=0.42446 AUC=0.962055 best=0.962148 lr=1.00e-04\n",
      "[fold 4 | ep 055] tr=0.44097 va=0.42572 AUC=0.961806 best=0.962148 lr=1.00e-04\n",
      "[fold 4 | ep 056] tr=0.44148 va=0.42415 AUC=0.962115 best=0.962148 lr=1.00e-04\n",
      "[fold 4 | ep 057] tr=0.44206 va=0.42594 AUC=0.961936 best=0.962148 lr=1.00e-04\n",
      "[fold 4 | ep 058] tr=0.44088 va=0.42416 AUC=0.962238 best=0.962238 lr=1.00e-04\n",
      "[fold 4 | ep 059] tr=0.44066 va=0.42406 AUC=0.962180 best=0.962238 lr=1.00e-04\n",
      "[fold 4 | ep 060] tr=0.43872 va=0.42498 AUC=0.962077 best=0.962238 lr=1.00e-04\n",
      "[fold 4 | ep 061] tr=0.43963 va=0.42520 AUC=0.962061 best=0.962238 lr=1.00e-04\n",
      "[fold 4 | ep 062] tr=0.43855 va=0.42508 AUC=0.961984 best=0.962238 lr=1.00e-04\n",
      "[fold 4 | ep 063] tr=0.44035 va=0.42447 AUC=0.962145 best=0.962238 lr=1.00e-04\n",
      "[fold 4 | ep 064] tr=0.43980 va=0.42440 AUC=0.962147 best=0.962238 lr=1.00e-04\n",
      "[fold 4 | ep 065] tr=0.43902 va=0.42425 AUC=0.962296 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 066] tr=0.43946 va=0.42505 AUC=0.962143 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 067] tr=0.43898 va=0.42471 AUC=0.962109 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 068] tr=0.43807 va=0.42419 AUC=0.962205 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 069] tr=0.43825 va=0.42458 AUC=0.962174 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 070] tr=0.43870 va=0.42561 AUC=0.962000 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 071] tr=0.43863 va=0.42548 AUC=0.962077 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 072] tr=0.43538 va=0.42531 AUC=0.962094 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 073] tr=0.43656 va=0.42462 AUC=0.962203 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 074] tr=0.43826 va=0.42523 AUC=0.962052 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 075] tr=0.43692 va=0.42586 AUC=0.962040 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 076] tr=0.43580 va=0.42633 AUC=0.961939 best=0.962296 lr=1.00e-04\n",
      "[fold 4 | ep 077] tr=0.43612 va=0.42585 AUC=0.962044 best=0.962296 lr=1.00e-04\n",
      "Early stopping at epoch 77. Best AUC: 0.962296\n",
      "[Interim] OOF ROC AUC using completed folds only will be printed at the end.\n",
      "\n",
      "OOF ROC AUC: 0.9619555770226316\n",
      "Submission saved to: runs/2025-08-12_10-17-24/submission.csv\n",
      "All artifacts saved under: runs/2025-08-12_10-17-24\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STRATIFIED K-FOLD TRAINING (Cosine + SWA + Live Tracker)\n",
    "# =========================\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "oof = np.zeros(len(train), dtype=np.float32)\n",
    "test_preds = np.zeros((len(test), N_SPLITS), dtype=np.float32)\n",
    "\n",
    "metrics_rows = []\n",
    "tracker = LiveTracker(LOGS_DIR, write_every=LIVE_WRITE_EVERY, live_print=LIVE_PRINT)\n",
    "\n",
    "pos_weight = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    pos_ratio = train[TARGET_COL].mean()\n",
    "    pos_weight_val = max(1e-6, (1.0 - pos_ratio) / max(1e-6, pos_ratio))\n",
    "    pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32, device=device)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(train[feature_cols], train[TARGET_COL])):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    t0 = time.time()\n",
    "    tr_df = train.iloc[tr_idx].reset_index(drop=True)\n",
    "    va_df = train.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "    tr_ds = TabDataset(tr_df, tr_df[TARGET_COL].values, num_cols, cat_cols)\n",
    "    va_ds = TabDataset(va_df, va_df[TARGET_COL].values, num_cols, cat_cols)\n",
    "    te_ds = TabDataset(test, None, num_cols, cat_cols)\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=pin_mem)\n",
    "    va_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=pin_mem)\n",
    "    te_loader = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=pin_mem)\n",
    "\n",
    "    model = TabularNN(num_dim=len(num_cols), cat_cardinalities=cat_cardinalities,\n",
    "                      hidden_layers=HIDDEN_LAYERS, dropout=DROPOUT, emb_dropout=EMB_DROPOUT, input_dropout=INPUT_DROPOUT).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=COSINE_T0, T_mult=COSINE_T_MULT, eta_min=MIN_LR)\n",
    "\n",
    "    # SWA setup\n",
    "    swa_model = AveragedModel(model) if USE_SWA else None\n",
    "    swa_scheduler = SWALR(optimizer, swa_lr=SWA_LR) if USE_SWA else None\n",
    "\n",
    "    early = EarlyStopper(patience=PATIENCE, mode=\"max\")\n",
    "    best_auc = -np.inf\n",
    "    fold_log = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_loss, _, _ = epoch_loop(model, tr_loader, criterion, optimizer=optimizer, clip_grad=0.5)  # lower gradient clipping\n",
    "        va_loss, va_pred, va_true = epoch_loop(model, va_loader, criterion, optimizer=None)\n",
    "        va_auc = roc_auc_score(va_true, va_pred)\n",
    "\n",
    "        # Scheduler step per epoch\n",
    "        scheduler.step(epoch + fold)  # add fold to decorrelate cycles a bit\n",
    "\n",
    "        # SWA step (after warmup epochs)\n",
    "        if USE_SWA and epoch >= SWA_START_EPOCH:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        else:\n",
    "            # keep base LR scheduler moving if not using SWA yet\n",
    "            pass\n",
    "\n",
    "        fold_log.append(dict(epoch=epoch, train_loss=tr_loss, val_loss=va_loss, val_auc=float(va_auc),\n",
    "                             lr=float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        tracker.update(\n",
    "            fold=fold, epoch=epoch, train_loss=tr_loss, val_loss=va_loss, val_auc=va_auc,\n",
    "            lr=optimizer.param_groups[0]['lr'], best_val_auc=max(best_auc, va_auc), t0=t0\n",
    "        )\n",
    "\n",
    "        improved = early.step(va_auc, model)\n",
    "        if improved:\n",
    "            best_auc = va_auc\n",
    "        if early.should_stop():\n",
    "            print(f\"Early stopping at epoch {epoch}. Best AUC: {best_auc:.6f}\")\n",
    "            break\n",
    "\n",
    "    # If SWA used: update BN stats and switch to SWA weights if better\n",
    "    if USE_SWA:\n",
    "        # Custom BN update for tabular model with dual inputs\n",
    "        swa_model.train()\n",
    "        with torch.no_grad():\n",
    "            for x_num, x_cat, _ in tr_loader:\n",
    "                x_num = x_num.to(device)\n",
    "                x_cat = x_cat.to(device)\n",
    "                swa_model(x_num, x_cat)\n",
    "        \n",
    "        # Evaluate SWA model\n",
    "        swa_model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds_swa = []\n",
    "            targs_swa = []\n",
    "            for x_num, x_cat, yb in va_loader:\n",
    "                x_num = x_num.to(device); x_cat = x_cat.to(device)\n",
    "                logits = swa_model(x_num, x_cat)\n",
    "                preds_swa.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                targs_swa.append(yb.numpy())\n",
    "            preds_swa = np.concatenate(preds_swa)\n",
    "            targs_swa = np.concatenate(targs_swa)\n",
    "            auc_swa = roc_auc_score(targs_swa, preds_swa)\n",
    "        if auc_swa >= best_auc:\n",
    "            best_auc = auc_swa\n",
    "            # copy SWA weights into base model\n",
    "            model.load_state_dict(swa_model.state_dict())\n",
    "            print(f\"SWA improved/kept AUC: {auc_swa:.6f}\")\n",
    "\n",
    "    # Save fold artifacts\n",
    "    fold_dir = FOLDS_DIR / f\"fold_{fold}\"\n",
    "    fold_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    log_df = pd.DataFrame(fold_log)\n",
    "    log_df.to_csv(fold_dir / \"train_log.csv\", index=False)\n",
    "    torch.save(model.state_dict(), fold_dir / \"model.pth\")\n",
    "\n",
    "    # Validation predictions\n",
    "    _, va_pred, va_true = epoch_loop(model, va_loader, criterion, optimizer=None)\n",
    "    oof[va_idx] = va_pred.squeeze()\n",
    "\n",
    "    # Test predictions\n",
    "    _, te_pred, _ = epoch_loop(model, te_loader, criterion, optimizer=None)\n",
    "    test_preds[:, fold] = te_pred.squeeze()\n",
    "\n",
    "    # Plots\n",
    "    fig_roc, ax = plt.subplots()\n",
    "    RocCurveDisplay.from_predictions(va_true, va_pred, ax=ax)\n",
    "    ax.set_title(f\"Fold {fold} ROC\")\n",
    "    fig_roc.savefig(fold_dir / \"roc_curve.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig_roc)\n",
    "\n",
    "    fig_lc, ax = plt.subplots()\n",
    "    ax.plot(log_df[\"epoch\"], log_df[\"train_loss\"], label=\"train_loss\")\n",
    "    ax.plot(log_df[\"epoch\"], log_df[\"val_loss\"], label=\"val_loss\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Loss\"); ax.set_title(f\"Fold {fold} Loss\"); ax.legend()\n",
    "    fig_lc.savefig(fold_dir / \"loss_curve.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig_lc)\n",
    "\n",
    "    fig_auc, ax = plt.subplots()\n",
    "    ax.plot(log_df[\"epoch\"], log_df[\"val_auc\"], label=\"val_auc\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"ROC AUC\"); ax.set_title(f\"Fold {fold} Val AUC\"); ax.legend()\n",
    "    fig_auc.savefig(fold_dir / \"val_auc_curve.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig_auc)\n",
    "\n",
    "    print(f\"[Interim] OOF ROC AUC using completed folds only will be printed at the end.\")\n",
    "\n",
    "# Final aggregate\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "# metrics_rows kept only per-fold best; compute now from oof\n",
    "overall_auc = roc_auc_score(train[TARGET_COL].values, oof)\n",
    "print(\"\\nOOF ROC AUC:\", overall_auc)\n",
    "\n",
    "# Save OOF + metrics\n",
    "if SAVE_OOF:\n",
    "    oof_df = pd.DataFrame({ID_COL: train[ID_COL].values, \"oof\": oof, TARGET_COL: train[TARGET_COL].values})\n",
    "    oof_df.to_csv(RUN_DIR / \"oof_predictions.csv\", index=False)\n",
    "\n",
    "pd.DataFrame(dict(fold=list(range(N_SPLITS)))).assign(best_val_auc=np.nan).to_csv(RUN_DIR / \"metrics.csv\", index=False)\n",
    "\n",
    "# Ensemble test preds\n",
    "test_pred_mean = test_preds.mean(axis=1)\n",
    "sub = pd.DataFrame({ID_COL: test[ID_COL].values, TARGET_COL: test_pred_mean})\n",
    "sub.to_csv(RUN_DIR / \"submission.csv\", index=False)\n",
    "print(\"Submission saved to:\", (RUN_DIR / \"submission.csv\").as_posix())\n",
    "\n",
    "# Overall plots\n",
    "fig_all, ax = plt.subplots()\n",
    "RocCurveDisplay.from_predictions(train[TARGET_COL].values, oof, ax=ax)\n",
    "ax.set_title(\"OOF ROC Curve\")\n",
    "fig_all.savefig(FIGS_DIR / \"oof_roc_curve.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig_all)\n",
    "\n",
    "fig_hist, ax = plt.subplots()\n",
    "ax.hist(oof, bins=50)\n",
    "ax.set_title(\"OOF Prediction Distribution\")\n",
    "ax.set_xlabel(\"Predicted probability\")\n",
    "fig_hist.savefig(FIGS_DIR / \"oof_pred_hist.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig_hist)\n",
    "\n",
    "with open(RUN_DIR / \"README.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"This folder contains outputs for a single run.\\n\"\n",
    "        \"- oof_predictions.csv: OOF probabilities with IDs and targets\\n\"\n",
    "        \"- submission.csv: ready for Kaggle submit\\n\"\n",
    "        \"- folds/*: per-fold model.pth, training logs, and plots (ROC, losses, AUC)\\n\"\n",
    "        \"- figs/*: overall figures (OOF ROC, hist)\\n\"\n",
    "        \"- logs/live_status.csv and logs/status.json: live tracking outputs\\n\"\n",
    "    )\n",
    "\n",
    "print(\"All artifacts saved under:\", RUN_DIR.as_posix())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
