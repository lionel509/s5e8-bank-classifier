{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4469c266",
   "metadata": {},
   "source": [
    "# Bagging Forests Category - Kaggle Playground Series S5E8\n",
    "\n",
    "**Category**: Bagging & Random Forests  \n",
    "**Sub-models**: RandomForestClassifier, ExtraTreesClassifier  \n",
    "**Split Strategy**: 70/30 stratified split  \n",
    "**Cross-Validation**: 5-fold StratifiedKFold  \n",
    "**Random Seed**: 42  \n",
    "**Artifact Paths**: outputs/bagging_forests/  \n",
    "\n",
    "This notebook compares ensemble tree-based models using bagging and random feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738dba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Bagging Forests Category - Setup Complete\n",
      "Bagging Forests Category - Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap installation and imports\n",
    "%pip install numpy pandas scikit-learn matplotlib --quiet\n",
    "\n",
    "import os, json, random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score, accuracy_score,\n",
    "    precision_score, recall_score, log_loss, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Bagging Forests Category - Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af5412b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (525000, 16) train, (225000, 16) test\n",
      "Features: 16\n",
      "Target distribution: [659512  90488]\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "train_df = pd.read_csv('../playground-series-s5e8/train.csv')\n",
    "test_df = pd.read_csv('../playground-series-s5e8/test.csv')\n",
    "\n",
    "feature_cols = [col for col in train_df.columns if col not in ['id', 'y']]\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['y']\n",
    "\n",
    "X_train_pool, X_test_holdout, y_train_pool, y_test_holdout = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(f\"Data loaded: {X_train_pool.shape} train, {X_test_holdout.shape} test\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83f1c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (750000, 18)\n",
      "Train columns: ['id', 'age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']\n",
      "\n",
      "Test data shape: (250000, 17)\n",
      "Test columns: ['id', 'age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome']\n",
      "\n",
      "First few rows of train data:\n",
      "   id  age          job  marital  education default  balance housing loan  \\\n",
      "0   0   42   technician  married  secondary      no        7      no   no   \n",
      "1   1   38  blue-collar  married  secondary      no      514      no   no   \n",
      "2   2   36  blue-collar  married  secondary      no      602     yes   no   \n",
      "3   3   27      student   single  secondary      no       34     yes   no   \n",
      "4   4   26   technician  married  secondary      no      889     yes   no   \n",
      "\n",
      "    contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
      "0  cellular   25   aug       117         3     -1         0  unknown  0  \n",
      "1   unknown   18   jun       185         1     -1         0  unknown  0  \n",
      "2   unknown   14   may       111         2     -1         0  unknown  0  \n",
      "3   unknown   28   may        10         2     -1         0  unknown  0  \n",
      "4  cellular    3   feb       902         1     -1         0  unknown  1  \n"
     ]
    }
   ],
   "source": [
    "# Explore data structure first\n",
    "train_df = pd.read_csv('../playground-series-s5e8/train.csv')\n",
    "test_df = pd.read_csv('../playground-series-s5e8/test.csv')\n",
    "\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Train columns:\", list(train_df.columns))\n",
    "print(\"\\nTest data shape:\", test_df.shape)  \n",
    "print(\"Test columns:\", list(test_df.columns))\n",
    "\n",
    "print(\"\\nFirst few rows of train data:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa934c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 4 ensemble tree variants:\n",
      "  - RandomForest_balanced\n",
      "  - RandomForest_default\n",
      "  - ExtraTrees_balanced\n",
      "  - ExtraTrees_default\n"
     ]
    }
   ],
   "source": [
    "# Define Random Forest and Extra Trees models\n",
    "models_config = {\n",
    "    'RandomForest_balanced': {\n",
    "        'estimator': RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [300, 800],\n",
    "            'classifier__max_depth': [None, 12, 20]\n",
    "        }\n",
    "    },\n",
    "    'RandomForest_default': {\n",
    "        'estimator': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [300, 800],\n",
    "            'classifier__max_depth': [None, 12, 20]\n",
    "        }\n",
    "    },\n",
    "    'ExtraTrees_balanced': {\n",
    "        'estimator': ExtraTreesClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [300, 800],\n",
    "            'classifier__max_depth': [None, 12, 20]\n",
    "        }\n",
    "    },\n",
    "    'ExtraTrees_default': {\n",
    "        'estimator': ExtraTreesClassifier(random_state=42, n_jobs=-1),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [300, 800],\n",
    "            'classifier__max_depth': [None, 12, 20]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(models_config)} ensemble tree variants:\")\n",
    "for name in models_config.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a63965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n",
      "Categorical columns: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
      "Numerical columns: ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def create_pipeline(estimator):\n",
    "    \"\"\"Create preprocessing pipeline - handle both categorical and numerical features\"\"\"\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    print(f\"    Pipeline setup:\")\n",
    "    print(f\"      Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    print(f\"      Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "    print(f\"      Preprocessing: SimpleImputer(median) + OneHotEncoder(drop_first)\")\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SimpleImputer(strategy='median'), numerical_cols),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', estimator)\n",
    "    ])\n",
    "    \n",
    "    print(f\"      Final pipeline: {' -> '.join(pipeline.named_steps.keys())}\")\n",
    "    return pipeline\n",
    "\n",
    "def get_probabilities(estimator, X):\n",
    "    \"\"\"Extract positive class probabilities from trained estimator\"\"\"\n",
    "    probs = estimator.predict_proba(X)[:, 1]\n",
    "    print(f\"      Generated probabilities: min={probs.min():.4f}, max={probs.max():.4f}, mean={probs.mean():.4f}\")\n",
    "    return probs\n",
    "\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    \"\"\"Compute comprehensive classification metrics\"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob),\n",
    "        'average_precision': average_precision_score(y_true, y_prob),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'logloss': log_loss(y_true, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def find_best_threshold(y_true, y_prob):\n",
    "    \"\"\"Find optimal threshold using Youden's J statistic\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    j_scores = tpr - fpr\n",
    "    best_idx = np.argmax(j_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_j_score = j_scores[best_idx]\n",
    "    \n",
    "    print(f\"      Optimal threshold: {best_threshold:.4f} (J-score: {best_j_score:.4f})\")\n",
    "    return best_threshold\n",
    "\n",
    "print(\"âœ… Helper functions defined with detailed logging\")\n",
    "print(\"\\nData preprocessing summary:\")\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "print(f\"  Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"  Numerical features ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"  Total original features: {len(categorical_cols) + len(numerical_cols)}\")\n",
    "print(f\"  Expected features after OneHot: {len(numerical_cols)} + (encoded categoricals)\")\n",
    "\n",
    "# Show unique values for categorical columns\n",
    "print(f\"\\nCategorical feature cardinality:\")\n",
    "for col in categorical_cols[:5]:  # Show first 5 to avoid too much output\n",
    "    n_unique = X[col].nunique()\n",
    "    print(f\"  {col}: {n_unique} unique values\")\n",
    "if len(categorical_cols) > 5:\n",
    "    print(f\"  ... and {len(categorical_cols) - 5} more categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84f79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: RandomForest_balanced\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best params: {'classifier__max_depth': None, 'classifier__n_estimators': 800}\n",
      "Best CV AUC: 0.9625\n",
      "Test AUC: 0.9625\n",
      "\n",
      "Evaluating: RandomForest_default\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "# Main evaluation loop\n",
    "results = {}\n",
    "\n",
    "print(\"Starting evaluation of all bagging forest models...\")\n",
    "print(f\"Total models to evaluate: {len(models_config)}\")\n",
    "print(f\"Cross-validation strategy: {cv.n_splits}-fold StratifiedKFold\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_idx, (model_name, config) in enumerate(models_config.items(), 1):\n",
    "    print(f\"\\n[{model_idx}/{len(models_config)}] Evaluating: {model_name}\")\n",
    "    print(f\"Estimator: {type(config['estimator']).__name__}\")\n",
    "    print(f\"Class weight: {getattr(config['estimator'], 'class_weight', 'default')}\")\n",
    "    \n",
    "    model_dir = f\"../outputs/bagging_forests/{model_name}\"\n",
    "    print(f\"Creating output directories: {model_dir}\")\n",
    "    os.makedirs(f\"{model_dir}/logs\", exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/models\", exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/figures\", exist_ok=True)\n",
    "    \n",
    "    print(\"Creating preprocessing pipeline...\")\n",
    "    pipeline = create_pipeline(config['estimator'])\n",
    "    print(f\"Pipeline steps: {list(pipeline.named_steps.keys())}\")\n",
    "    \n",
    "    print(\"Starting grid search...\")\n",
    "    print(f\"Parameter grid: {config['param_grid']}\")\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, config['param_grid'], cv=cv, \n",
    "        scoring='roc_auc', n_jobs=1, verbose=1  # n_jobs=1 to avoid nested parallelism\n",
    "    )\n",
    "    \n",
    "    print(\"Fitting grid search on training data...\")\n",
    "    grid_search.fit(X_train_pool, y_train_pool)\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Grid search completed!\")\n",
    "    print(f\"Best params: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV AUC: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # CV analysis\n",
    "    print(\"\\nStarting detailed cross-validation analysis...\")\n",
    "    cv_metrics = []\n",
    "    cv_roc_curves = []\n",
    "    cv_pr_curves = []\n",
    "    cv_thresholds = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train_pool, y_train_pool)):\n",
    "        print(f\"  Processing fold {fold_idx + 1}/{cv.n_splits}...\")\n",
    "        \n",
    "        X_fold_train = X_train_pool.iloc[train_idx]\n",
    "        X_fold_val = X_train_pool.iloc[val_idx]\n",
    "        y_fold_train = y_train_pool.iloc[train_idx]\n",
    "        y_fold_val = y_train_pool.iloc[val_idx]\n",
    "        \n",
    "        print(f\"    Train size: {len(X_fold_train)}, Val size: {len(X_fold_val)}\")\n",
    "        print(f\"    Target distribution - Train: {np.bincount(y_fold_train)}, Val: {np.bincount(y_fold_val)}\")\n",
    "        \n",
    "        fold_pipeline = create_pipeline(config['estimator'])\n",
    "        fold_pipeline.set_params(**grid_search.best_params_)\n",
    "        \n",
    "        print(f\"    Training fold model...\")\n",
    "        fold_pipeline.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        print(f\"    Making predictions...\")\n",
    "        y_val_prob = get_probabilities(fold_pipeline, X_fold_val)\n",
    "        best_threshold = find_best_threshold(y_fold_val, y_val_prob)\n",
    "        cv_thresholds.append(best_threshold)\n",
    "        \n",
    "        print(f\"    Computing metrics with threshold: {best_threshold:.4f}\")\n",
    "        fold_metrics = compute_metrics(y_fold_val, y_val_prob, best_threshold)\n",
    "        fold_metrics['fold'] = fold_idx + 1\n",
    "        fold_metrics['threshold'] = best_threshold\n",
    "        cv_metrics.append(fold_metrics)\n",
    "        \n",
    "        print(f\"    Fold {fold_idx + 1} AUC: {fold_metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        print(f\"    Computing ROC and PR curves...\")\n",
    "        fpr, tpr, _ = roc_curve(y_fold_val, y_val_prob)\n",
    "        precision, recall, _ = precision_recall_curve(y_fold_val, y_val_prob)\n",
    "        cv_roc_curves.append((fpr, tpr))\n",
    "        cv_pr_curves.append((precision, recall))\n",
    "    \n",
    "    # Test evaluation\n",
    "    print(\"\\nEvaluating on test holdout set...\")\n",
    "    mean_threshold = np.mean(cv_thresholds)\n",
    "    print(f\"Using mean threshold from CV: {mean_threshold:.4f}\")\n",
    "    \n",
    "    y_test_prob = get_probabilities(best_pipeline, X_test_holdout)\n",
    "    print(f\"Generated {len(y_test_prob)} test predictions\")\n",
    "    \n",
    "    test_metrics = compute_metrics(y_test_holdout, y_test_prob, mean_threshold)\n",
    "    test_metrics['chosen_threshold'] = mean_threshold\n",
    "    \n",
    "    print(\"Computing confusion matrix...\")\n",
    "    test_metrics['confusion_matrix'] = confusion_matrix(\n",
    "        y_test_holdout, (y_test_prob >= mean_threshold).astype(int)\n",
    "    ).tolist()\n",
    "    \n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  Average Precision: {test_metrics['average_precision']:.4f}\")\n",
    "    print(f\"  F1 Score: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    print(\"\\nStoring results and saving artifacts...\")\n",
    "    results[model_name] = {\n",
    "        'cv_metrics': cv_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_roc_curves': cv_roc_curves,\n",
    "        'cv_pr_curves': cv_pr_curves,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'model_dir': model_dir\n",
    "    }\n",
    "    \n",
    "    # Save artifacts\n",
    "    print(\"Saving CV metrics to CSV...\")\n",
    "    cv_df = pd.DataFrame(cv_metrics)\n",
    "    cv_df.to_csv(f\"{model_dir}/logs/cv_metrics.csv\", index=False)\n",
    "    \n",
    "    print(\"Saving test metrics to JSON...\")\n",
    "    with open(f\"{model_dir}/logs/test_metrics.json\", 'w') as f:\n",
    "        json.dump(test_metrics, f, indent=2)\n",
    "    \n",
    "    print(\"Saving final model...\")\n",
    "    with open(f\"{model_dir}/models/final_model.pkl\", 'wb') as f:\n",
    "        pickle.dump(best_pipeline, f)\n",
    "    \n",
    "    print(f\"Model {model_name} evaluation completed!\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nðŸŽ‰ All forest models evaluated successfully!\")\n",
    "print(f\"Results stored for {len(results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1492f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figures with forest-specific plots\n",
    "print(\"Starting figure generation for all models...\")\n",
    "print(f\"Total models to generate figures for: {len(results)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_idx, (model_name, model_results) in enumerate(results.items(), 1):\n",
    "    model_dir = model_results['model_dir']\n",
    "    \n",
    "    print(f\"\\n[{model_idx}/{len(results)}] Generating figures for {model_name}...\")\n",
    "    print(f\"Output directory: {model_dir}/figures/\")\n",
    "    \n",
    "    # Load final model\n",
    "    print(\"Loading saved model...\")\n",
    "    with open(f\"{model_dir}/models/final_model.pkl\", 'rb') as f:\n",
    "        final_model = pickle.load(f)\n",
    "    \n",
    "    forest_estimator = final_model.named_steps['classifier']\n",
    "    print(f\"Model type: {type(forest_estimator).__name__}\")\n",
    "    print(f\"Number of estimators: {forest_estimator.n_estimators}\")\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    print(\"Extracting feature names after preprocessing...\")\n",
    "    try:\n",
    "        # Get feature names from the preprocessor\n",
    "        preprocessor = final_model.named_steps['preprocessor']\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        print(f\"Successfully extracted {len(feature_names)} feature names\")\n",
    "        print(f\"Sample feature names: {feature_names[:5]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract feature names: {e}\")\n",
    "        # Fallback: create generic names\n",
    "        n_features = len(forest_estimator.feature_importances_)\n",
    "        feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "        print(f\"Using {len(feature_names)} generic feature names\")\n",
    "    \n",
    "    # 1. Feature importance\n",
    "    print(\"\\nGenerating feature importance plot...\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    importances = forest_estimator.feature_importances_\n",
    "    print(f\"Feature importances shape: {importances.shape}\")\n",
    "    print(f\"Top 5 importance values: {sorted(importances, reverse=True)[:5]}\")\n",
    "    \n",
    "    indices = np.argsort(importances)[-20:]  # Top 20\n",
    "    print(f\"Plotting top 20 features (indices: {indices[:5]}...{indices[-5:]})\")\n",
    "    \n",
    "    plt.barh(range(len(indices)), importances[indices], alpha=0.7)\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Feature Importance (Gini Impurity Reduction)')\n",
    "    plt.title(f'Top 20 Feature Importances - {model_name}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    importance_path = f\"{model_dir}/figures/feature_importance.png\"\n",
    "    plt.savefig(importance_path, dpi=200, bbox_inches='tight')\n",
    "    print(f\"Saved feature importance plot: {importance_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    print(\"Generating ROC curve plot...\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    \n",
    "    print(f\"Processing {len(model_results['cv_roc_curves'])} CV fold ROC curves...\")\n",
    "    for fold_idx, (fpr, tpr) in enumerate(model_results['cv_roc_curves']):\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        if fold_idx < 3:  # Show details for first 3 folds\n",
    "            auc_fold = np.trapz(tpr, fpr)\n",
    "            print(f\"  Fold {fold_idx + 1} AUC: {auc_fold:.4f}\")\n",
    "    \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    mean_auc = np.mean([cv[\"roc_auc\"] for cv in model_results[\"cv_metrics\"]])\n",
    "    \n",
    "    print(f\"Mean CV AUC: {mean_auc:.4f}\")\n",
    "    print(f\"ROC curve std range: {std_tpr.min():.4f} to {std_tpr.max():.4f}\")\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, 'b-', \n",
    "             label=f'Mean ROC (AUC = {mean_auc:.3f})')\n",
    "    plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, alpha=0.2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    roc_path = f\"{model_dir}/figures/roc_cv.png\"\n",
    "    plt.savefig(roc_path, dpi=200, bbox_inches='tight')\n",
    "    print(f\"Saved ROC curve plot: {roc_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"âœ… Completed figures for {model_name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nðŸŽ¨ All figures generated successfully!\")\n",
    "print(\"Figure types created for each model:\")\n",
    "print(\"  - Feature importance (top 20 features)\")\n",
    "print(\"  - ROC curves with confidence intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary\n",
    "print(\"Creating comprehensive summary of all model results...\")\n",
    "print(f\"Processing results for {len(results)} models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_data = []\n",
    "for model_name, model_results in results.items():\n",
    "    print(f\"\\nProcessing summary for: {model_name}\")\n",
    "    \n",
    "    test_metrics = model_results['test_metrics']\n",
    "    cv_metrics = model_results['cv_metrics']\n",
    "    \n",
    "    # Calculate CV statistics\n",
    "    cv_aucs = [cv['roc_auc'] for cv in cv_metrics]\n",
    "    cv_auc_mean = np.mean(cv_aucs)\n",
    "    cv_auc_std = np.std(cv_aucs)\n",
    "    \n",
    "    print(f\"  Test AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  CV AUC: {cv_auc_mean:.4f} Â± {cv_auc_std:.4f}\")\n",
    "    print(f\"  Test F1: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"  Best params: {model_results['best_params']}\")\n",
    "    \n",
    "    summary_data.append({\n",
    "        'model': model_name,\n",
    "        'test_auc': test_metrics['roc_auc'],\n",
    "        'test_ap': test_metrics['average_precision'],\n",
    "        'test_f1': test_metrics['f1'],\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'test_precision': test_metrics['precision'],\n",
    "        'test_recall': test_metrics['recall'],\n",
    "        'cv_auc_mean': cv_auc_mean,\n",
    "        'cv_auc_std': cv_auc_std,\n",
    "        'best_params': str(model_results['best_params']),\n",
    "        'artifacts_path': model_results['model_dir']\n",
    "    })\n",
    "\n",
    "print(f\"\\nCreating summary DataFrame with {len(summary_data)} models...\")\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('test_auc', ascending=False)\n",
    "\n",
    "print(\"Summary DataFrame columns:\")\n",
    "for col in summary_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nSaving summary to CSV...\")\n",
    "os.makedirs('../outputs/bagging_forests', exist_ok=True)\n",
    "summary_path = '../outputs/bagging_forests/summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ† BAGGING FORESTS CATEGORY - FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel Performance Ranking (by Test AUC):\")\n",
    "print(\"-\" * 50)\n",
    "for idx, row in summary_df.iterrows():\n",
    "    rank = summary_df.index.get_loc(idx) + 1\n",
    "    print(f\"{rank:2d}. {row['model']:25s} | AUC: {row['test_auc']:.4f} | AP: {row['test_ap']:.4f} | F1: {row['test_f1']:.4f}\")\n",
    "\n",
    "best_model = summary_df.iloc[0]\n",
    "print(f\"\\nðŸ¥‡ Best Model: {best_model['model']}\")\n",
    "print(f\"   Test AUC: {best_model['test_auc']:.4f}\")\n",
    "print(f\"   Test AP:  {best_model['test_ap']:.4f}\")\n",
    "print(f\"   Test F1:  {best_model['test_f1']:.4f}\")\n",
    "print(f\"   CV AUC:   {best_model['cv_auc_mean']:.4f} Â± {best_model['cv_auc_std']:.4f}\")\n",
    "print(f\"   Best params: {best_model['best_params']}\")\n",
    "\n",
    "print(f\"\\nðŸ“ All artifacts saved in: ../outputs/bagging_forests/\")\n",
    "print(\"   - Individual model results in subdirectories\")\n",
    "print(\"   - Consolidated summary in summary.csv\")\n",
    "print(\"   - Feature importance and ROC plots for each model\")\n",
    "\n",
    "print(f\"\\nâœ¨ Analysis complete! Processed {len(results)} forest models successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
