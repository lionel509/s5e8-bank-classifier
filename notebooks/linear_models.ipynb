{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8acb2093",
   "metadata": {},
   "source": [
    "# Linear Models Category - Kaggle Playground Series S5E8\n",
    "\n",
    "**Category**: Linear Models  \n",
    "**Sub-models**: LogisticRegression (lbfgs, saga + L1/L2/ElasticNet), RidgeClassifier, SGDClassifier, Perceptron, LinearSVC  \n",
    "**Split Strategy**: 70/30 stratified split  \n",
    "**Cross-Validation**: 5-fold StratifiedKFold  \n",
    "**Random Seed**: 42  \n",
    "**Artifact Paths**: outputs/linear_models/  \n",
    "\n",
    "This notebook compares multiple linear model variants using the same data preprocessing and evaluation protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap installation and imports\n",
    "%pip install numpy pandas scikit-learn matplotlib shap xgboost lightgbm catboost tensorflow --quiet\n",
    "\n",
    "import os, json, random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score, accuracy_score,\n",
    "    precision_score, recall_score, log_loss, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, calibration_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Linear Models Category - Setup Complete\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('../playground-series-s5e8/train.csv')\n",
    "test_df = pd.read_csv('../playground-series-s5e8/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "print(f\"Target distribution: {train_df['target'].value_counts().to_dict()}\")\n",
    "\n",
    "# Features and target\n",
    "feature_cols = [col for col in train_df.columns if col not in ['id', 'target']]\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['target']\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Feature columns: {feature_cols[:10]}...\")  # Show first 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split: 70% train_pool, 30% test_holdout\n",
    "X_train_pool, X_test_holdout, y_train_pool, y_test_holdout = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train pool: {X_train_pool.shape}\")\n",
    "print(f\"Test holdout: {X_test_holdout.shape}\")\n",
    "print(f\"Train pool target distribution: {y_train_pool.value_counts().to_dict()}\")\n",
    "print(f\"Test holdout target distribution: {y_test_holdout.value_counts().to_dict()}\")\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"\\nCross-validation setup: 5-fold StratifiedKFold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear models and their hyperparameter grids\n",
    "models_config = {\n",
    "    'LogisticRegression_lbfgs': {\n",
    "        'estimator': LogisticRegression(solver='lbfgs', max_iter=2000, random_state=42),\n",
    "        'param_grid': {\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "            'classifier__penalty': ['l2', 'none']\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression_saga_l1': {\n",
    "        'estimator': LogisticRegression(solver='saga', max_iter=2000, random_state=42),\n",
    "        'param_grid': {\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "            'classifier__penalty': ['l1']\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression_saga_elasticnet': {\n",
    "        'estimator': LogisticRegression(solver='saga', max_iter=2000, random_state=42),\n",
    "        'param_grid': {\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "            'classifier__penalty': ['elasticnet'],\n",
    "            'classifier__l1_ratio': [0.5]\n",
    "        }\n",
    "    },\n",
    "    'RidgeClassifier': {\n",
    "        'estimator': RidgeClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'classifier__alpha': [0.1, 1.0, 10.0]\n",
    "        }\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'estimator': SGDClassifier(loss='log_loss', random_state=42),\n",
    "        'param_grid': {\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "            'classifier__l1_ratio': [0.0, 0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'Perceptron': {\n",
    "        'estimator': Perceptron(random_state=42),\n",
    "        'param_grid': {\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    'LinearSVC': {\n",
    "        'estimator': CalibratedClassifierCV(\n",
    "            LinearSVC(random_state=42, max_iter=2000), \n",
    "            method='sigmoid', cv=3\n",
    "        ),\n",
    "        'param_grid': {\n",
    "            'classifier__base_estimator__C': [0.1, 1, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(models_config)} linear model variants:\")\n",
    "for name in models_config.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def create_pipeline(estimator):\n",
    "    \"\"\"Create a preprocessing pipeline with the given estimator\"\"\"\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', estimator)\n",
    "    ])\n",
    "\n",
    "def get_probabilities(estimator, X):\n",
    "    \"\"\"Get probabilities from estimator, handling different probability methods\"\"\"\n",
    "    if hasattr(estimator, 'predict_proba'):\n",
    "        return estimator.predict_proba(X)[:, 1]\n",
    "    elif hasattr(estimator, 'decision_function'):\n",
    "        # Convert decision function to probabilities using sigmoid\n",
    "        decision = estimator.decision_function(X)\n",
    "        return 1 / (1 + np.exp(-decision))\n",
    "    else:\n",
    "        raise ValueError(\"Estimator doesn't support probability prediction\")\n",
    "\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    \"\"\"Compute all evaluation metrics\"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob),\n",
    "        'average_precision': average_precision_score(y_true, y_prob),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'logloss': log_loss(y_true, y_prob)\n",
    "    }\n",
    "\n",
    "def find_best_threshold(y_true, y_prob):\n",
    "    \"\"\"Find best threshold using Youden's J statistic\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    j_scores = tpr - fpr\n",
    "    best_idx = np.argmax(j_scores)\n",
    "    return thresholds[best_idx]\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "results = {}\n",
    "all_cv_results = []\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create directories for this model\n",
    "    model_dir = f\"../outputs/linear_models/{model_name}\"\n",
    "    os.makedirs(f\"{model_dir}/logs\", exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/models\", exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/figures\", exist_ok=True)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = create_pipeline(config['estimator'])\n",
    "    \n",
    "    # Cross-validation with hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, config['param_grid'], cv=cv, \n",
    "        scoring='roc_auc', n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit on train pool\n",
    "    grid_search.fit(X_train_pool, y_train_pool)\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV AUC: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Collect CV results for detailed analysis\n",
    "    cv_metrics = []\n",
    "    cv_roc_curves = []\n",
    "    cv_pr_curves = []\n",
    "    cv_thresholds = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train_pool, y_train_pool)):\n",
    "        X_fold_train = X_train_pool.iloc[train_idx]\n",
    "        X_fold_val = X_train_pool.iloc[val_idx]\n",
    "        y_fold_train = y_train_pool.iloc[train_idx]\n",
    "        y_fold_val = y_train_pool.iloc[val_idx]\n",
    "        \n",
    "        # Fit best pipeline on this fold\n",
    "        fold_pipeline = create_pipeline(config['estimator'])\n",
    "        fold_pipeline.set_params(**grid_search.best_params_)\n",
    "        fold_pipeline.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Predict on validation fold\n",
    "        y_val_prob = get_probabilities(fold_pipeline, X_fold_val)\n",
    "        \n",
    "        # Find best threshold for this fold\n",
    "        best_threshold = find_best_threshold(y_fold_val, y_val_prob)\n",
    "        cv_thresholds.append(best_threshold)\n",
    "        \n",
    "        # Compute metrics\n",
    "        fold_metrics = compute_metrics(y_fold_val, y_val_prob, best_threshold)\n",
    "        fold_metrics['fold'] = fold_idx + 1\n",
    "        fold_metrics['threshold'] = best_threshold\n",
    "        cv_metrics.append(fold_metrics)\n",
    "        \n",
    "        # Store curves for plotting\n",
    "        fpr, tpr, _ = roc_curve(y_fold_val, y_val_prob)\n",
    "        precision, recall, _ = precision_recall_curve(y_fold_val, y_val_prob)\n",
    "        cv_roc_curves.append((fpr, tpr))\n",
    "        cv_pr_curves.append((precision, recall))\n",
    "        \n",
    "        print(f\"  Fold {fold_idx + 1}: AUC={fold_metrics['roc_auc']:.4f}, \"\n",
    "              f\"AP={fold_metrics['average_precision']:.4f}, \"\n",
    "              f\"F1={fold_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Calculate mean threshold from CV\n",
    "    mean_threshold = np.mean(cv_thresholds)\n",
    "    \n",
    "    # Evaluate on test holdout using mean threshold\n",
    "    y_test_prob = get_probabilities(best_pipeline, X_test_holdout)\n",
    "    test_metrics = compute_metrics(y_test_holdout, y_test_prob, mean_threshold)\n",
    "    test_metrics['chosen_threshold'] = mean_threshold\n",
    "    test_metrics['confusion_matrix'] = confusion_matrix(\n",
    "        y_test_holdout, (y_test_prob >= mean_threshold).astype(int)\n",
    "    ).tolist()\n",
    "    \n",
    "    print(f\"\\nTest Results (threshold={mean_threshold:.4f}):\")\n",
    "    print(f\"  AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  AP: {test_metrics['average_precision']:.4f}\")\n",
    "    print(f\"  F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'cv_metrics': cv_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_roc_curves': cv_roc_curves,\n",
    "        'cv_pr_curves': cv_pr_curves,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'model_dir': model_dir\n",
    "    }\n",
    "    \n",
    "    # Add to overall results\n",
    "    all_cv_results.extend([{\n",
    "        'model': model_name,\n",
    "        'category': 'linear_models',\n",
    "        **metrics\n",
    "    } for metrics in cv_metrics])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All models evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b922b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts for each model\n",
    "for model_name, model_results in results.items():\n",
    "    model_dir = model_results['model_dir']\n",
    "    \n",
    "    # Save CV metrics\n",
    "    cv_df = pd.DataFrame(model_results['cv_metrics'])\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_stats = []\n",
    "    for metric in ['roc_auc', 'average_precision', 'f1', 'accuracy', 'precision', 'recall', 'logloss']:\n",
    "        summary_stats.append({\n",
    "            'fold': 'mean',\n",
    "            metric: cv_df[metric].mean(),\n",
    "            'threshold': cv_df['threshold'].mean()\n",
    "        })\n",
    "        summary_stats.append({\n",
    "            'fold': 'std',\n",
    "            metric: cv_df[metric].std(),\n",
    "            'threshold': cv_df['threshold'].std()\n",
    "        })\n",
    "    \n",
    "    cv_summary_df = pd.concat([cv_df, pd.DataFrame(summary_stats)], ignore_index=True)\n",
    "    cv_summary_df.to_csv(f\"{model_dir}/logs/cv_metrics.csv\", index=False)\n",
    "    \n",
    "    # Save test metrics\n",
    "    with open(f\"{model_dir}/logs/test_metrics.json\", 'w') as f:\n",
    "        json.dump(model_results['test_metrics'], f, indent=2)\n",
    "    \n",
    "    # Save model\n",
    "    # Refit best pipeline on full train pool\n",
    "    final_pipeline = create_pipeline(models_config[model_name]['estimator'])\n",
    "    grid_search_final = GridSearchCV(\n",
    "        final_pipeline, models_config[model_name]['param_grid'], \n",
    "        cv=cv, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    grid_search_final.fit(X_train_pool, y_train_pool)\n",
    "    \n",
    "    with open(f\"{model_dir}/models/final_model.pkl\", 'wb') as f:\n",
    "        pickle.dump(grid_search_final.best_estimator_, f)\n",
    "    \n",
    "    print(f\"Artifacts saved for {model_name}\")\n",
    "\n",
    "print(\"\\nAll artifacts saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figures for each model\n",
    "for model_name, model_results in results.items():\n",
    "    model_dir = model_results['model_dir']\n",
    "    \n",
    "    print(f\"Generating figures for {model_name}...\")\n",
    "    \n",
    "    # 1. ROC Curve with CV mean and std\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot individual fold curves in light color\n",
    "    for i, (fpr, tpr) in enumerate(model_results['cv_roc_curves']):\n",
    "        plt.plot(fpr, tpr, alpha=0.3, color='gray')\n",
    "    \n",
    "    # Calculate mean ROC curve\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    for fpr, tpr in model_results['cv_roc_curves']:\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, 'b-', label=f'Mean ROC (AUC = {np.mean([cv[\"roc_auc\"] for cv in model_results[\"cv_metrics\"]]):.3f} ± {np.std([cv[\"roc_auc\"] for cv in model_results[\"cv_metrics\"]]):.3f})')\n",
    "    plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, alpha=0.2, color='blue')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/roc_cv.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Precision-Recall Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot individual fold curves\n",
    "    for i, (precision, recall) in enumerate(model_results['cv_pr_curves']):\n",
    "        plt.plot(recall, precision, alpha=0.3, color='gray')\n",
    "    \n",
    "    # Calculate mean PR curve (simplified)\n",
    "    mean_ap = np.mean([cv['average_precision'] for cv in model_results['cv_metrics']])\n",
    "    std_ap = np.std([cv['average_precision'] for cv in model_results['cv_metrics']])\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {model_name}\\nAP = {mean_ap:.3f} ± {std_ap:.3f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/pr_cv.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Load final model for remaining plots\n",
    "    with open(f\"{model_dir}/models/final_model.pkl\", 'rb') as f:\n",
    "        final_model = pickle.load(f)\n",
    "    \n",
    "    y_test_prob = get_probabilities(final_model, X_test_holdout)\n",
    "    \n",
    "    # 4. Calibration curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    try:\n",
    "        prob_true, prob_pred = calibration_curve(y_test_holdout, y_test_prob, n_bins=10)\n",
    "        plt.plot(prob_pred, prob_true, 'o-', label='Calibration curve')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "        plt.xlabel('Mean Predicted Probability')\n",
    "        plt.ylabel('Fraction of Positives')\n",
    "        plt.title(f'Calibration Curve - {model_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    except:\n",
    "        plt.text(0.5, 0.5, 'Calibration curve not available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f'Calibration Curve - {model_name}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/calibration_curve.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Confusion Matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    \n",
    "    cm = model_results['test_metrics']['confusion_matrix']\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Negative', 'Positive'])\n",
    "    plt.yticks(tick_marks, ['Negative', 'Positive'])\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i][j]), ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/confusion_matrix.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Feature importance (for linear models - coefficient magnitudes)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    try:\n",
    "        if hasattr(final_model.named_steps['classifier'], 'coef_'):\n",
    "            coef = final_model.named_steps['classifier'].coef_[0]\n",
    "            abs_coef = np.abs(coef)\n",
    "            \n",
    "            # Get top 20 features\n",
    "            top_indices = np.argsort(abs_coef)[-20:]\n",
    "            top_coef = coef[top_indices]\n",
    "            top_features = [feature_cols[i] for i in top_indices]\n",
    "            \n",
    "            colors = ['red' if c < 0 else 'blue' for c in top_coef]\n",
    "            plt.barh(range(len(top_coef)), top_coef, color=colors, alpha=0.7)\n",
    "            plt.yticks(range(len(top_features)), top_features)\n",
    "            plt.xlabel('Coefficient Value')\n",
    "            plt.title(f'Top 20 Feature Coefficients - {model_name}')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Feature importance not available', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title(f'Feature Importance - {model_name}')\n",
    "    except Exception as e:\n",
    "        plt.text(0.5, 0.5, f'Feature importance error: {str(e)}', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f'Feature Importance - {model_name}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/feature_importance.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 7. Error analysis\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    errors = np.abs(y_test_holdout - y_test_prob)\n",
    "    plt.scatter(y_test_prob, errors, alpha=0.6)\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Prediction Error')\n",
    "    plt.title(f'Error Analysis - {model_name}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/error_analysis.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(\"All figures generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table and final results\n",
    "summary_data = []\n",
    "\n",
    "for model_name, model_results in results.items():\n",
    "    test_metrics = model_results['test_metrics']\n",
    "    cv_metrics = model_results['cv_metrics']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'model': model_name,\n",
    "        'test_auc': test_metrics['roc_auc'],\n",
    "        'test_ap': test_metrics['average_precision'],\n",
    "        'test_f1': test_metrics['f1'],\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'cv_auc_mean': np.mean([cv['roc_auc'] for cv in cv_metrics]),\n",
    "        'cv_auc_std': np.std([cv['roc_auc'] for cv in cv_metrics]),\n",
    "        'best_params': str(model_results['best_params']),\n",
    "        'artifacts_path': model_results['model_dir']\n",
    "    })\n",
    "\n",
    "# Sort by test AUC\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('test_auc', ascending=False)\n",
    "\n",
    "# Save summary\n",
    "os.makedirs('../outputs/linear_models', exist_ok=True)\n",
    "summary_df.to_csv('../outputs/linear_models/summary.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LINEAR MODELS CATEGORY - FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRanked by Test AUC:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for idx, row in summary_df.iterrows():\n",
    "    print(f\"{row['model']:30s} | AUC: {row['test_auc']:.4f} | AP: {row['test_ap']:.4f} | F1: {row['test_f1']:.4f}\")\n",
    "    print(f\"{'':30s} | CV AUC: {row['cv_auc_mean']:.4f}±{row['cv_auc_std']:.4f}\")\n",
    "    print(f\"{'':30s} | Artifacts: {row['artifacts_path']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nBest Model: {summary_df.iloc[0]['model']}\")\n",
    "print(f\"Best Test AUC: {summary_df.iloc[0]['test_auc']:.4f}\")\n",
    "print(f\"\\nAll results saved to: ../outputs/linear_models/\")\n",
    "print(f\"Summary saved to: ../outputs/linear_models/summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
