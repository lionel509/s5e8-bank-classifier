{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfbbac5",
   "metadata": {},
   "source": [
    "# Aggregate Leaderboards - Kaggle Playground Series S5E8\n",
    "\n",
    "This notebook scans all category results and creates:\n",
    "1. Per-category summary (best model per category)\n",
    "2. Overall leaderboard (all models ranked by test AUC)\n",
    "3. Summary README with analysis\n",
    "\n",
    "**Categories**: linear_models, svm_kernels, knn, naive_bayes, trees, bagging_forests, boosting_gbms, neural_nets  \n",
    "**Output**: summary/per_category.csv, summary/overall_leaderboard.csv, summary/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os, json, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Aggregate Leaderboards - Setup Complete\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan outputs directory for all categories and models\n",
    "outputs_dir = Path('../outputs')\n",
    "categories = [d.name for d in outputs_dir.iterdir() if d.is_dir()]\n",
    "categories = sorted(categories)\n",
    "\n",
    "print(f\"Found {len(categories)} categories:\")\n",
    "for cat in categories:\n",
    "    print(f\"  - {cat}\")\n",
    "\n",
    "all_results = []\n",
    "category_summaries = []\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\nProcessing category: {category}\")\n",
    "    category_dir = outputs_dir / category\n",
    "    \n",
    "    # Get all models in this category\n",
    "    model_dirs = [d for d in category_dir.iterdir() if d.is_dir() and d.name != 'summary']\n",
    "    \n",
    "    category_results = []\n",
    "    \n",
    "    for model_dir in model_dirs:\n",
    "        model_name = model_dir.name\n",
    "        \n",
    "        # Try to load test metrics\n",
    "        test_metrics_file = model_dir / 'logs' / 'test_metrics.json'\n",
    "        cv_metrics_file = model_dir / 'logs' / 'cv_metrics.csv'\n",
    "        \n",
    "        if test_metrics_file.exists():\n",
    "            with open(test_metrics_file, 'r') as f:\n",
    "                test_metrics = json.load(f)\n",
    "            \n",
    "            # Load CV metrics for additional stats\n",
    "            cv_stats = {}\n",
    "            if cv_metrics_file.exists():\n",
    "                cv_df = pd.read_csv(cv_metrics_file)\n",
    "                # Filter out summary rows (mean/std)\n",
    "                cv_data = cv_df[cv_df['fold'].apply(lambda x: str(x).isdigit())]\n",
    "                if len(cv_data) > 0:\n",
    "                    cv_stats = {\n",
    "                        'cv_auc_mean': cv_data['roc_auc'].mean(),\n",
    "                        'cv_auc_std': cv_data['roc_auc'].std(),\n",
    "                        'cv_ap_mean': cv_data['average_precision'].mean(),\n",
    "                        'cv_ap_std': cv_data['average_precision'].std(),\n",
    "                        'cv_f1_mean': cv_data['f1'].mean(),\n",
    "                        'cv_f1_std': cv_data['f1'].std()\n",
    "                    }\n",
    "            \n",
    "            # Combine all metrics\n",
    "            result = {\n",
    "                'category': category,\n",
    "                'model': model_name,\n",
    "                'test_auc': test_metrics.get('roc_auc', 0),\n",
    "                'test_ap': test_metrics.get('average_precision', 0),\n",
    "                'test_f1': test_metrics.get('f1', 0),\n",
    "                'test_accuracy': test_metrics.get('accuracy', 0),\n",
    "                'test_precision': test_metrics.get('precision', 0),\n",
    "                'test_recall': test_metrics.get('recall', 0),\n",
    "                'test_logloss': test_metrics.get('logloss', np.inf),\n",
    "                'chosen_threshold': test_metrics.get('chosen_threshold', 0.5),\n",
    "                'artifacts_path': str(model_dir),\n",
    "                **cv_stats\n",
    "            }\n",
    "            \n",
    "            all_results.append(result)\n",
    "            category_results.append(result)\n",
    "            \n",
    "            print(f\"  {model_name}: AUC={result['test_auc']:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {model_name}: No test metrics found\")\n",
    "    \n",
    "    # Find best model in this category\n",
    "    if category_results:\n",
    "        best_model = max(category_results, key=lambda x: x['test_auc'])\n",
    "        category_summaries.append({\n",
    "            'category': category,\n",
    "            'best_model': best_model['model'],\n",
    "            'best_test_auc': best_model['test_auc'],\n",
    "            'best_test_ap': best_model['test_ap'],\n",
    "            'best_test_f1': best_model['test_f1'],\n",
    "            'cv_auc_mean': best_model.get('cv_auc_mean', 0),\n",
    "            'cv_auc_std': best_model.get('cv_auc_std', 0),\n",
    "            'num_models': len(category_results),\n",
    "            'artifacts_path': best_model['artifacts_path']\n",
    "        })\n",
    "        print(f\"  Best: {best_model['model']} (AUC: {best_model['test_auc']:.4f})\")\n",
    "\n",
    "print(f\"\\nTotal models found: {len(all_results)}\")\n",
    "print(f\"Categories processed: {len(category_summaries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdebb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overall leaderboard\n",
    "overall_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Sort by test AUC (primary), then AP (secondary), then F1 (tertiary)\n",
    "overall_df = overall_df.sort_values(\n",
    "    ['test_auc', 'test_ap', 'test_f1'], \n",
    "    ascending=[False, False, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "overall_df['rank'] = range(1, len(overall_df) + 1)\n",
    "\n",
    "print(\"\\nOVERALL LEADERBOARD (Top 10)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Rank':<4} {'Category':<15} {'Model':<25} {'AUC':<8} {'AP':<8} {'F1':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in overall_df.head(10).iterrows():\n",
    "    print(f\"{row['rank']:<4} {row['category']:<15} {row['model']:<25} {row['test_auc']:<8.4f} {row['test_ap']:<8.4f} {row['test_f1']:<8.4f}\")\n",
    "\n",
    "# Save overall leaderboard\n",
    "os.makedirs('../summary', exist_ok=True)\n",
    "overall_df.to_csv('../summary/overall_leaderboard.csv', index=False)\n",
    "print(f\"\\nOverall leaderboard saved to: ../summary/overall_leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-category summary\n",
    "category_df = pd.DataFrame(category_summaries)\n",
    "category_df = category_df.sort_values('best_test_auc', ascending=False).reset_index(drop=True)\n",
    "category_df['category_rank'] = range(1, len(category_df) + 1)\n",
    "\n",
    "print(\"\\nPER-CATEGORY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Rank':<4} {'Category':<15} {'Best Model':<25} {'AUC':<8} {'AP':<8} {'F1':<8} {'#Models':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in category_df.iterrows():\n",
    "    print(f\"{row['category_rank']:<4} {row['category']:<15} {row['best_model']:<25} {row['best_test_auc']:<8.4f} {row['best_test_ap']:<8.4f} {row['best_test_f1']:<8.4f} {row['num_models']:<8}\")\n",
    "\n",
    "# Save per-category summary\n",
    "category_df.to_csv('../summary/per_category.csv', index=False)\n",
    "print(f\"\\nPer-category summary saved to: ../summary/per_category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate analysis and insights\n",
    "best_overall = overall_df.iloc[0]\n",
    "best_category = category_df.iloc[0]\n",
    "\n",
    "# Performance statistics\n",
    "auc_stats = overall_df['test_auc'].describe()\n",
    "category_performance = category_df.groupby('category')['best_test_auc'].first().sort_values(ascending=False)\n",
    "\n",
    "# Category analysis\n",
    "category_analysis = []\n",
    "for category in categories:\n",
    "    cat_results = [r for r in all_results if r['category'] == category]\n",
    "    if cat_results:\n",
    "        aucs = [r['test_auc'] for r in cat_results]\n",
    "        category_analysis.append({\n",
    "            'category': category,\n",
    "            'mean_auc': np.mean(aucs),\n",
    "            'std_auc': np.std(aucs),\n",
    "            'min_auc': np.min(aucs),\n",
    "            'max_auc': np.max(aucs),\n",
    "            'num_models': len(aucs)\n",
    "        })\n",
    "\n",
    "analysis_df = pd.DataFrame(category_analysis).sort_values('mean_auc', ascending=False)\n",
    "\n",
    "print(\"\\nCATEGORY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Category':<15} {'Mean AUC':<10} {'Std AUC':<10} {'Min AUC':<10} {'Max AUC':<10} {'Models':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in analysis_df.iterrows():\n",
    "    print(f\"{row['category']:<15} {row['mean_auc']:<10.4f} {row['std_auc']:<10.4f} {row['min_auc']:<10.4f} {row['max_auc']:<10.4f} {row['num_models']:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51814e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive README\n",
    "readme_content = f\"\"\"# ML Benchmarking Results - Kaggle Playground Series S5E8\n",
    "\n",
    "## Overview\n",
    "\n",
    "This document summarizes the comprehensive ML benchmarking results across {len(categories)} model categories and {len(all_results)} total model configurations.\n",
    "\n",
    "**Dataset**: Kaggle Playground Series S5E8  \n",
    "**Evaluation Protocol**: 70/30 stratified split, 5-fold CV on train pool, final test on holdout  \n",
    "**Primary Metric**: ROC-AUC  \n",
    "**Secondary Metrics**: Average Precision, F1-Score  \n",
    "\n",
    "## 🏆 Best Overall Results\n",
    "\n",
    "| Rank | Category | Model | Test AUC | Test AP | Test F1 |\n",
    "|------|----------|-------|----------|---------|----------|\n",
    "\"\"\"\n",
    "\n",
    "# Add top 10 results\n",
    "for idx, row in overall_df.head(10).iterrows():\n",
    "    readme_content += f\"| {row['rank']} | {row['category']} | {row['model']} | {row['test_auc']:.4f} | {row['test_ap']:.4f} | {row['test_f1']:.4f} |\\n\"\n",
    "\n",
    "readme_content += f\"\"\"\n",
    "## 📊 Category Performance Summary\n",
    "\n",
    "| Rank | Category | Best Model | Test AUC | Models Tested |\n",
    "|------|----------|------------|----------|---------------|\n",
    "\"\"\"\n",
    "\n",
    "# Add category summary\n",
    "for idx, row in category_df.iterrows():\n",
    "    readme_content += f\"| {row['category_rank']} | {row['category']} | {row['best_model']} | {row['best_test_auc']:.4f} | {row['num_models']} |\\n\"\n",
    "\n",
    "readme_content += f\"\"\"\n",
    "## 📈 Statistical Analysis\n",
    "\n",
    "### Overall Performance Statistics\n",
    "- **Best AUC**: {auc_stats['max']:.4f} ({best_overall['category']}/{best_overall['model']})\n",
    "- **Mean AUC**: {auc_stats['mean']:.4f}\n",
    "- **Median AUC**: {auc_stats['50%']:.4f}\n",
    "- **Std AUC**: {auc_stats['std']:.4f}\n",
    "- **Models Evaluated**: {len(all_results)}\n",
    "\n",
    "### Category Analysis\n",
    "\n",
    "| Category | Mean AUC | Std AUC | Best AUC | Models |\n",
    "|----------|----------|---------|----------|--------|\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in analysis_df.iterrows():\n",
    "    readme_content += f\"| {row['category']} | {row['mean_auc']:.4f} | {row['std_auc']:.4f} | {row['max_auc']:.4f} | {row['num_models']} |\\n\"\n",
    "\n",
    "readme_content += f\"\"\"\n",
    "## 🎯 Key Insights\n",
    "\n",
    "1. **Best Performing Category**: {best_category['category']} (AUC: {best_category['best_test_auc']:.4f})\n",
    "2. **Most Consistent Category**: {analysis_df.loc[analysis_df['std_auc'].idxmin(), 'category']} (Std: {analysis_df['std_auc'].min():.4f})\n",
    "3. **Best Single Model**: {best_overall['model']} from {best_overall['category']} (AUC: {best_overall['test_auc']:.4f})\n",
    "\n",
    "## 📁 File Structure\n",
    "\n",
    "```\n",
    "outputs/\n",
    "├── <category>/\n",
    "│   ├── <model>/\n",
    "│   │   ├── logs/\n",
    "│   │   │   ├── cv_metrics.csv\n",
    "│   │   │   └── test_metrics.json\n",
    "│   │   ├── models/\n",
    "│   │   │   └── final_model.pkl\n",
    "│   │   └── figures/\n",
    "│   │       ├── roc_cv.png\n",
    "│   │       ├── pr_cv.png\n",
    "│   │       ├── confusion_matrix.png\n",
    "│   │       ├── calibration_curve.png\n",
    "│   │       ├── feature_importance.png\n",
    "│   │       └── error_analysis.png\n",
    "│   └── summary.csv\n",
    "└── summary/\n",
    "    ├── overall_leaderboard.csv\n",
    "    ├── per_category.csv\n",
    "    └── README.md\n",
    "```\n",
    "\n",
    "## 🔍 Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "- Median imputation for missing values\n",
    "- StandardScaler for distance-based models (linear, SVM, KNN, neural nets)\n",
    "- No scaling for tree-based models (trees, forests, boosting)\n",
    "\n",
    "### Evaluation Protocol\n",
    "1. **Data Split**: 70% train pool, 30% test holdout (stratified)\n",
    "2. **Cross-Validation**: 5-fold StratifiedKFold on train pool\n",
    "3. **Hyperparameter Tuning**: GridSearchCV with ROC-AUC scoring\n",
    "4. **Threshold Selection**: Youden's J statistic from CV folds\n",
    "5. **Final Evaluation**: Single evaluation on test holdout\n",
    "\n",
    "### Metrics Collected\n",
    "- ROC-AUC (primary ranking metric)\n",
    "- Average Precision (tie-breaker)\n",
    "- F1-Score (tie-breaker)\n",
    "- Accuracy, Precision, Recall, Log-Loss\n",
    "- Confusion Matrix, Calibration Analysis\n",
    "\n",
    "---\n",
    "*Generated automatically by aggregate_leaderboards.ipynb*\n",
    "\"\"\"\n",
    "\n",
    "# Save README\n",
    "with open('../summary/README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"\\nComprehensive README generated: ../summary/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n📊 SUMMARY:\")\n",
    "print(f\"   • Categories processed: {len(categories)}\")\n",
    "print(f\"   • Total models evaluated: {len(all_results)}\")\n",
    "print(f\"   • Best overall model: {best_overall['model']} ({best_overall['category']})\")\n",
    "print(f\"   • Best overall AUC: {best_overall['test_auc']:.4f}\")\n",
    "print(f\"   • Best category: {best_category['category']} (AUC: {best_category['best_test_auc']:.4f})\")\n",
    "\n",
    "print(f\"\\n📁 FILES GENERATED:\")\n",
    "print(f\"   • summary/overall_leaderboard.csv - All models ranked by AUC\")\n",
    "print(f\"   • summary/per_category.csv - Best model per category\")\n",
    "print(f\"   • summary/README.md - Comprehensive analysis and insights\")\n",
    "\n",
    "print(f\"\\n🏆 TOP 3 MODELS:\")\n",
    "for i in range(min(3, len(overall_df))):\n",
    "    row = overall_df.iloc[i]\n",
    "    print(f\"   {i+1}. {row['model']} ({row['category']}) - AUC: {row['test_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Aggregation notebook completed successfully!\")\n",
    "print(f\"   Check summary/ folder for detailed results and analysis.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
