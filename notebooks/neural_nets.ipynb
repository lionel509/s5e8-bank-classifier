{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37148f49",
   "metadata": {},
   "source": [
    "# Neural Networks Category - Kaggle Playground Series S5E8\n",
    "\n",
    "**Category**: Neural Networks  \n",
    "**Sub-models**: Keras MLP variants (shallow, medium, deep) with different architectures  \n",
    "**Split Strategy**: 70/30 stratified split  \n",
    "**Cross-Validation**: 5-fold StratifiedKFold  \n",
    "**Random Seed**: 42  \n",
    "**Artifact Paths**: outputs/neural_nets/  \n",
    "\n",
    "This notebook compares different neural network architectures using Keras/TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392966e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap installation and imports\n",
    "%pip install numpy pandas scikit-learn matplotlib tensorflow --quiet\n",
    "\n",
    "import os, json, random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score, accuracy_score,\n",
    "    precision_score, recall_score, log_loss, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, calibration_curve\n",
    ")\n",
    "\n",
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Neural Networks Category - Setup Complete\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ccee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "train_df = pd.read_csv('../playground-series-s5e8/train.csv')\n",
    "test_df = pd.read_csv('../playground-series-s5e8/test.csv')\n",
    "\n",
    "feature_cols = [col for col in train_df.columns if col not in ['id', 'target']]\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['target']\n",
    "\n",
    "X_train_pool, X_test_holdout, y_train_pool, y_test_holdout = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(f\"Data loaded: {X_train_pool.shape} train, {X_test_holdout.shape} test\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "\n",
    "# Preprocessing for neural networks\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_processed = scaler.fit_transform(imputer.fit_transform(X_train_pool))\n",
    "X_test_processed = scaler.transform(imputer.transform(X_test_holdout))\n",
    "\n",
    "n_features = X_train_processed.shape[1]\n",
    "print(f\"Processed features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df2576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architectures\n",
    "def create_shallow_model(input_dim, activation='relu', use_batch_norm=False, dropout_rate=0.3):\n",
    "    \"\"\"Shallow network: 64 -> 32\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=activation, input_shape=(input_dim,))\n",
    "    ])\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(32, activation=activation))\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def create_medium_model(input_dim, activation='relu', use_batch_norm=False, dropout_rate=0.3):\n",
    "    \"\"\"Medium network: 256 -> 128 -> 64\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(256, activation=activation, input_shape=(input_dim,))\n",
    "    ])\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(128, activation=activation))\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(64, activation=activation))\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def create_deep_model(input_dim, activation='relu', use_batch_norm=False, dropout_rate=0.3):\n",
    "    \"\"\"Deep network: 512 -> 256 -> 128 -> 64\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation=activation, input_shape=(input_dim,))\n",
    "    ])\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(256, activation=activation))\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(128, activation=activation))\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(64, activation=activation))\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    if dropout_rate > 0:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Define model configurations\n",
    "models_config = {\n",
    "    'MLP_shallow_relu': {\n",
    "        'create_fn': create_shallow_model,\n",
    "        'activation': 'relu',\n",
    "        'batch_norm': False,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'MLP_shallow_relu_bn': {\n",
    "        'create_fn': create_shallow_model,\n",
    "        'activation': 'relu',\n",
    "        'batch_norm': True,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'MLP_medium_relu': {\n",
    "        'create_fn': create_medium_model,\n",
    "        'activation': 'relu',\n",
    "        'batch_norm': False,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'MLP_medium_swish': {\n",
    "        'create_fn': create_medium_model,\n",
    "        'activation': 'swish',\n",
    "        'batch_norm': True,\n",
    "        'dropout': 0.5\n",
    "    },\n",
    "    'MLP_deep_relu': {\n",
    "        'create_fn': create_deep_model,\n",
    "        'activation': 'relu',\n",
    "        'batch_norm': True,\n",
    "        'dropout': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(models_config)} neural network variants:\")\n",
    "for name in models_config.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdb833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_probabilities(model, X):\n",
    "    return model.predict(X, verbose=0).flatten()\n",
    "\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return {\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob),\n",
    "        'average_precision': average_precision_score(y_true, y_prob),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'logloss': log_loss(y_true, y_prob)\n",
    "    }\n",
    "\n",
    "def find_best_threshold(y_true, y_prob):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    j_scores = tpr - fpr\n",
    "    best_idx = np.argmax(j_scores)\n",
    "    return thresholds[best_idx]\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "results = {}\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\nEvaluating: {model_name}\")\n",
    "    \n",
    "    model_dir = f\"../outputs/neural_nets/{model_name}\"\n",
    "    os.makedirs(f\"{model_dir}/logs\", exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/models\", exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/figures\", exist_ok=True)\n",
    "    \n",
    "    # CV analysis\n",
    "    cv_metrics = []\n",
    "    cv_roc_curves = []\n",
    "    cv_pr_curves = []\n",
    "    cv_thresholds = []\n",
    "    cv_histories = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train_pool, y_train_pool)):\n",
    "        print(f\"  Fold {fold_idx + 1}/5\")\n",
    "        \n",
    "        # Split fold data\n",
    "        X_fold_train = X_train_processed[train_idx]\n",
    "        X_fold_val = X_train_processed[val_idx]\n",
    "        y_fold_train = y_train_pool.iloc[train_idx].values\n",
    "        y_fold_val = y_train_pool.iloc[val_idx].values\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = config['create_fn'](\n",
    "            input_dim=n_features,\n",
    "            activation=config['activation'],\n",
    "            use_batch_norm=config['batch_norm'],\n",
    "            dropout_rate=config['dropout']\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['AUC']\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True, monitor='val_auc', mode='max'),\n",
    "            ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            validation_data=(X_fold_val, y_fold_val),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        cv_histories.append(history.history)\n",
    "        \n",
    "        # Predict on validation fold\n",
    "        y_val_prob = get_probabilities(model, X_fold_val)\n",
    "        \n",
    "        # Find best threshold for this fold\n",
    "        best_threshold = find_best_threshold(y_fold_val, y_val_prob)\n",
    "        cv_thresholds.append(best_threshold)\n",
    "        \n",
    "        # Compute metrics\n",
    "        fold_metrics = compute_metrics(y_fold_val, y_val_prob, best_threshold)\n",
    "        fold_metrics['fold'] = fold_idx + 1\n",
    "        fold_metrics['threshold'] = best_threshold\n",
    "        cv_metrics.append(fold_metrics)\n",
    "        \n",
    "        # Store curves for plotting\n",
    "        fpr, tpr, _ = roc_curve(y_fold_val, y_val_prob)\n",
    "        precision, recall, _ = precision_recall_curve(y_fold_val, y_val_prob)\n",
    "        cv_roc_curves.append((fpr, tpr))\n",
    "        cv_pr_curves.append((precision, recall))\n",
    "        \n",
    "        print(f\"    AUC: {fold_metrics['roc_auc']:.4f}, AP: {fold_metrics['average_precision']:.4f}\")\n",
    "    \n",
    "    # Train final model on full train pool\n",
    "    print(\"  Training final model...\")\n",
    "    final_model = config['create_fn'](\n",
    "        input_dim=n_features,\n",
    "        activation=config['activation'],\n",
    "        use_batch_norm=config['batch_norm'],\n",
    "        dropout_rate=config['dropout']\n",
    "    )\n",
    "    \n",
    "    final_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    final_history = final_model.fit(\n",
    "        X_train_processed, y_train_pool.values,\n",
    "        epochs=50,  # Reduced epochs for final model\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Test evaluation\n",
    "    mean_threshold = np.mean(cv_thresholds)\n",
    "    y_test_prob = get_probabilities(final_model, X_test_processed)\n",
    "    test_metrics = compute_metrics(y_test_holdout, y_test_prob, mean_threshold)\n",
    "    test_metrics['chosen_threshold'] = mean_threshold\n",
    "    test_metrics['confusion_matrix'] = confusion_matrix(\n",
    "        y_test_holdout, (y_test_prob >= mean_threshold).astype(int)\n",
    "    ).tolist()\n",
    "    \n",
    "    print(f\"  Test AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'cv_metrics': cv_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_roc_curves': cv_roc_curves,\n",
    "        'cv_pr_curves': cv_pr_curves,\n",
    "        'cv_histories': cv_histories,\n",
    "        'final_history': final_history.history,\n",
    "        'model_dir': model_dir\n",
    "    }\n",
    "    \n",
    "    # Save artifacts\n",
    "    cv_df = pd.DataFrame(cv_metrics)\n",
    "    cv_df.to_csv(f\"{model_dir}/logs/cv_metrics.csv\", index=False)\n",
    "    \n",
    "    with open(f\"{model_dir}/logs/test_metrics.json\", 'w') as f:\n",
    "        json.dump(test_metrics, f, indent=2)\n",
    "    \n",
    "    # Save model in TensorFlow format\n",
    "    final_model.save(f\"{model_dir}/models/final_model.keras\")\n",
    "\n",
    "print(\"\\nAll neural network models evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figures with NN-specific plots\n",
    "for model_name, model_results in results.items():\n",
    "    model_dir = model_results['model_dir']\n",
    "    \n",
    "    print(f\"Generating figures for {model_name}...\")\n",
    "    \n",
    "    # 1. Learning curves (training history)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    final_history = model_results['final_history']\n",
    "    plt.plot(final_history['loss'], label='Training Loss')\n",
    "    plt.title(f'Training Loss - {model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary Crossentropy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # AUC plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(final_history['auc'], label='Training AUC')\n",
    "    plt.title(f'Training AUC - {model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/learning_curve.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    for fpr, tpr in model_results['cv_roc_curves']:\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, 'b-', \n",
    "             label=f'Mean ROC (AUC = {np.mean([cv[\"roc_auc\"] for cv in model_results[\"cv_metrics\"]]):.3f})')\n",
    "    plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, alpha=0.2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/roc_cv.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Feature importance placeholder (would require permutation importance)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.text(0.5, 0.5, 'Feature importance for neural networks\\nrequires permutation importance\\n(computationally expensive)', \n",
    "            ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title(f'Feature Importance - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_dir}/figures/feature_importance.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(\"All figures generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79169384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary\n",
    "summary_data = []\n",
    "for model_name, model_results in results.items():\n",
    "    test_metrics = model_results['test_metrics']\n",
    "    cv_metrics = model_results['cv_metrics']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'model': model_name,\n",
    "        'test_auc': test_metrics['roc_auc'],\n",
    "        'test_ap': test_metrics['average_precision'],\n",
    "        'test_f1': test_metrics['f1'],\n",
    "        'cv_auc_mean': np.mean([cv['roc_auc'] for cv in cv_metrics]),\n",
    "        'cv_auc_std': np.std([cv['roc_auc'] for cv in cv_metrics]),\n",
    "        'artifacts_path': model_results['model_dir']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('test_auc', ascending=False)\n",
    "os.makedirs('../outputs/neural_nets', exist_ok=True)\n",
    "summary_df.to_csv('../outputs/neural_nets/summary.csv', index=False)\n",
    "\n",
    "print(\"\\nNEURAL NETWORKS CATEGORY - FINAL RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "for idx, row in summary_df.iterrows():\n",
    "    print(f\"{row['model']:25s} | AUC: {row['test_auc']:.4f} | AP: {row['test_ap']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Model: {summary_df.iloc[0]['model']} (AUC: {summary_df.iloc[0]['test_auc']:.4f})\")\n",
    "print(f\"Summary saved to: ../outputs/neural_nets/summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
